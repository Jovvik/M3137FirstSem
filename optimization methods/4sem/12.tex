\chapter{21 апреля}

\begin{example}
    \textcolor{red}{Опущено.}
\end{example}

\subsubsection{Нормы и анализ ошибок}

\[||A|| = \sum_{i = 1}^n \sum_{j = 1}^n |a_{ij}|\]

\[||Ax|| \leq ||A|| \cdot ||x||\]
Можем выразить другим образом:
\[||A|| = M = \max_{x \neq 0} \frac{||Ax||}{||x||}\]
\[||A|| = \max_j ||a_j||\]

\begin{lemma}[результат Уилкинсона]
    Вычисленное решение \(x^*\) удовлетворяет системе \((A + E)x^* = b\), где элементы \(E \) имеют уровень ошибок округления.
\end{lemma}

\begin{align*}
    (A + E)x^*                               & = b                                      \\
    b - Ax^*                                 & = Ex^*                                   \\
    ||b - Ax^*||                             & = ||Ex^*|| \leq ||E|| \cdot ||x^*||      \\
    \frac{||b - Ax^*||}{||A|| \cdot ||x^*||} & \leq C \cdot \varepsilon_{\text{машины}}
\end{align*}

Если \(A\) не вырождена, то:
\begin{align*}
    x - x^*                     & = A^{ - 1} (b - Ax)                                                     \\
    ||x - x^*||                 & \leq ||A^{ - 1}|| \cdot ||E|| \cdot ||x^*||                             \\
    \frac{||x - x^*||}{||x^*||} & \leq C \cdot ||A|| \cdot ||A^{ - 1}|| \cdot \varepsilon_{\text{машины}} \\
\end{align*}

Т.к. \(||A^{ - 1}|| = \frac{1}{m}\), то:
\[\cond(A) = ||A|| \cdot ||A^{ - 1}||\]
И таким образом:
\[\frac{||x - x^*||}{||x^*||} \leq C \cdot \cond(A) \cdot \varepsilon_{\text{машины}}\]

Можно заметить, что вычисление \(\cond(A)\) требует вычисление обратной матрицы. Это можно несколько упростить, заметив, что если \(a_j\) --- столбцы \(A\), \(\tilde{a}_j\) --- столбцы матрицы \(A^{ - 1}\), то:
\[\cond(A) = \max_j ||a_j|| \cdot \max_j ||\tilde{a}_j||\]
Тем не менее, такое вычисление примерно утраивает время вычисления. Но на практике точное значение \(\cond(A)\) не требуется и используются приблизительные оценки.

\subsubsection{Оценивание числа обусловленности}

\[||A^{ - 1}|| = \frac{1}{\min_x \frac{||Ax||}{||x||}} = \max_x \frac{||x||}{||Ax||} = \max_y \frac{||A^{ - 1}y||}{||y||}\]

Решим систему \(Az = y\), тогда
\[\frac{||z||}{||y||} = \underbrace{\frac{||A^{ - 1}y||}{||y||}}_{\text{оценка} ||A^{-1}||}\]
Но если брать произвольный \(y\), то оценка может быть неточной. Будем использовать такой \(y\), что \(A^T y = C\), где \(C \) --- вектор с компонентами \( \pm 1\).

\begin{example}
    \textcolor{red}{Опущено.}
\end{example}

\section{Дополнительно о градиентных методах}

Релаксационная последовательность задается рекурентно как \(x^k = x^{k - 1} + \alpha_k u^k, k \in N, u^k \in E_n\). Условие спуска при этом \(\ev{\nabla f(x), u} < 0\).

Какое брать \(\alpha_k\)? Такое, чтобы выполнялось следующее:
\begin{equation}
    f(x^{k - 1}) + \alpha_k u^k \leq (1 - \lambda_k) f(x^{k - 1}) + \lambda_k \min_{\alpha \in E} f(x^{k - 1} + \alpha u^k)
    \label{условие alpha}
\end{equation}

Очевидно, что \(\lambda_k \in [0, 1]\). Чтобы оценить это соотношение, используются эвристические приёмы. В частности, если
\[f(x^{k - 1} + \alpha_k u^k) \leq f(x^{k - 1})\]
, то \(\{x_k\}\) будет релаксационной. Несложно заметить, что мы рассмотрели \eqref{условие alpha} для случая \(\lambda_k = 0\).

Если \(\lambda_k = 1\), то для нахождения наилучшего значения \(\alpha_k^*\) необходимо решить задачу одномерной минимизации, что мы делали в лабораторной работе.

Если \(\lambda_k \in (0, 1)\), то:
\[f(x^{k - 1}) - f(x^k) \geq \lambda_k (f(x^{k - 1}) - f(x^{k - 1} + \alpha_k^* u^k))\]
Это равносильно \eqref{условие alpha} и из этого можно предположить, что \(\lambda_k\) характеризует наименьшую долю из максимально возможного уменьшения \(f(x)\) вдоль направления \(u^k\), которое должна обеспечивать релаксационная последовательность \(\{x_k\}\).

Будем обозначать антиградиент как \(\omega(x) = - \nabla f(x)\)

\subsection{Метод градиентного спуска}

\[x^k = x^{k - 1} + \beta_k \underbrace{\frac{\omega^k}{|\omega^k|}}_{u_k}\]
\[\beta_k = \underbrace{\alpha}_{\const} |\omega^k|\]
Тогда можно переписать:

\[x^k = x^{k - 1} + \alpha_k \omega^k\]
Один из главных недостатков градиентного спуска состоит в том, что в окрестности стационарной точки \(\tilde{x}\) шаг может оказаться слишком большим, и тогда метод ``проскочит'' \(\tilde{x}\). Шаг также может быть настолько большим, что произойдёт \(f(x^k) > f(x^{k - 1})\) и последовательность перестанет быть релаксационной. Можно уменьшить шаг, но тогда замедлится сходимость релаксационной последовательности. Все эти проблемы сводятся к задаче оценки возможной величины \(\alpha\), которая обеспечивала бы высокую скорость сходимости без проскакивания стационарной точки.

\begin{theorem}
    \label{релаксационная функция}
    Пусть \(f(x)\) ограничена снизу и дифференцируема в пространстве \(E_n\), а её градиент удовлетворяет условию Липшица, т.е.:
    \[\forall x, y \in E_n \ \ |\nabla f(x) - \nabla f(y)| \leq L|x - y|\]
    , где \(L > 0\) --- \(\const\).

    Тогда \(\{x_k\} : x^k = x^{k - 1} + \alpha \omega^k\) с \(\alpha \in (0, \frac{2}{L})\) является релаксационной. При этом справедлива оценка:
    \[f(x^k) \leq f(x^{k - 1}) - \alpha \left( 1 - \frac{\alpha L}{2}  \right) |\nabla f(x^{k - 1})|^2\]
    и \(|\nabla f(x^k)| \to 0\) при \(k \to +\infty\).
\end{theorem}

Мы уже рассматривали схожую теорему для квадратичных функций. В том случае \(L = \max \lambda_i\), где \(\{\lambda_i\}\) --- собственные значения. Эти теоремы согласованы.

Если \(f(x)\) удовлетворяет теореме \ref{релаксационная функция}, то при \(\lambda = \frac{1}{L}\) \(\{x_k\}\) --- релаксационная последовательность и не происходит ``проскакивание'' стационарной точки.
\[||\omega^k|| = ||\nabla f(x^{k - 1})|| \leq L \cdot |x^{k - 1} - \tilde{x}|\]
Следовательно, при \(\alpha \leq \frac{1}{L}\):
\[|x^k - x^{k - 1}| = \alpha |\omega^k| \leq |x^{k - 1} - \tilde{x}|\]

Пусть \(f(x)\) ограничена снизу, а \(\{x^k\}\) такое, что \(\exists \gamma_0 > 0\):

\begin{align}
    f(x^{k - 1}) - f(x^k) & \geq \gamma_0 |\omega^k| ^2 \label{условие брух}     \\
    f(x^0) - f(x^m)       & \geq \gamma_0 \sum_{k = 1}^m |\nabla f(x^{k - 1})|^2
\end{align}

\[\sum_{k = 1}^{+\infty} |\nabla f(x^{k - 1})|^2 \text{ --- знакоположительный сходящийся ряд}\]

\(\nabla f(x^k) \to 0\) при \(k \to +\infty\). Таким образом, при построении \(\{x_k\}\) если удастся выполнить условие \eqref{условие брух}, то последовательность градиентов \( \to 0\), а следовательно \(\{x_k\}\) будет сходиться к стационарной точке.

Найти значение константы \(L\) в реальных функция бывает трудно, если не невозможно. Тогда градиентный метод не даст гарантий, что он сойдётся. В таком случае модифицируем рекурентное соотношение:
\begin{equation}
    x^k = x^{k - 1} + \alpha_k \omega^k
    \label{измененная рекурента}
\end{equation}
, т.е. \(\alpha\) меняется на каждом шаге. Есть разные методы выбора \(\alpha\), например:
\[\varphi_k(\alpha) := f(x^{k - 1} + \alpha \omega^k)\]
\[\varphi_k'(0) = \ev{\nabla f(x), \omega^k} = - |\omega^k|^2\]
В окрестности \(\alpha = 0\) \(\varphi'_k\) убывает до \(\alpha_k^*\).

Если рассматривать \(\alpha_k = \alpha_k^*\), то такой метод называется \textbf{исчерпывающий спуск}. При этом если \(f(x)\) удовлетворяет теореме \ref{релаксационная функция}, то \(\{x_k\}\) по исчерпывающему спуску удовлетворяет условиям \eqref{условие брух}.
\[\varphi_k'(\alpha) = \ev{\nabla f(x^{k - 1} + \alpha \omega^k), \omega^k}\]
\[\ev{\omega^{k + 1}, \omega^k} = \ev{ -\nabla f(x^k), \omega^k} = - \varphi_k' (\alpha^*_k) = 0\]
\begin{align*}
    ||\omega^k||^2 & = \ev{\omega^k, \omega^k - \omega^{k + 1}}               \\
                   & \leq |\omega^k| \cdot |\omega^k - \omega^{k + 1}|        \\
                   & = |\omega^k| \cdot |\nabla f(x^{k - 1}) - \nabla f(x^k)| \\
                   & \leq L \cdot |\omega^k| \cdot |x^{k - 1} - x^k|          \\
                   & = L \cdot \alpha_k^* \cdot |\omega^k|^2
\end{align*}

Таким образом:
\[\alpha_k^* \geq \frac{1}{L}\]
\[f(x^{k - 1}) - f(x^k) \geq f(x^{k - 1}) - f(\tilde{x}^k) \geq \frac{1}{2L} |\omega^k|^2\]
\[\tilde{x}^k = x^{k - 1} + \underbrace{\frac{1}{L}}_\alpha \omega^k\]