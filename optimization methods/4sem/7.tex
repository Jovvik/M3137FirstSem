\chapter{22 марта \textit{(дополнительная лекция)}}

\begin{definition}
    Направление вектора \(p^k\) называется \textbf{направлением убывания} функции \(f(x)\) в точке \(x^k\), если при всех достаточно малых положительных \(\alpha\) выполняется неравенство \(f(x^k + \alpha p^k) < f(x^k)\)
\end{definition}

\begin{theorem}[достаточное условие направления убывания]
    Пусть функция \(f(x)\) дифференцируема в точке \(x^k\). Если вектор \(p^k\) удовлетворяет условию \(\ev{\nabla f(x^k), p^k} < 0\), то направление вектора \(p^k\) является направлением убывания.
\end{theorem}
\begin{proof}
    Из свойства дифференцируемости функции и условия теоремы следует, что
    \[f(x^{k + 1}) - f(x^k) = f(x^k + \alpha p^k) - f(x^k) = \ev{\nabla f(x^k), \alpha p^k} + \smallO(\alpha) = \alpha(\nabla f(x^k), p^k + \frac{\smallO(\alpha)}{\alpha} ) < 0\]
    при всех достаточно малых \(\alpha > 0\), т.е. \(p^k\) задает направление убывания \(f(x)\) в точке \(x^k\).
\end{proof}

Геометрическая интерпретация: \(\ev{\nabla f(x^k), p^k} < 0 \Rightarrow p^k\) составляет тупой угол с \(\nabla f(x^k)\).

Рассмотрим \(f(x)\), дифференцируемую в \(E_n\) и запишем итерационную процедуру минимизации:
\begin{equation}
    x^{k + 1} = x^k + \alpha_k p^k
    \label{итерационный процесс}
\end{equation}
, где \(p^k\) определяется с учетом информации о частных производных, а величина \(\alpha_k\) такова, что:
\begin{equation}
    f(x^{k + 1}) < f(x^k)
    \label{уменьшение f}
\end{equation}

Условие остановки итерационного процесса: \(||\nabla f(x^k)|| < \varepsilon\).

\subsection{Метод градиентного спуска}

Предпололжим, что в~\eqref{итерационный процесс} \(p^k = - \nabla f(x^k)\). Если \(\nabla f(x^k) \neq 0\), то \(\ev{\nabla f(x^k), p^k} < 0\), следовательно \(p^k\) --- направление убывания \(f(x)\), причём в малой окрестности точки \(x^k\) направление \(p^k\) обеспечивает наискорейшее \textbf{убывание} функции. Таким образом, \(\exists \alpha_k > 0\), такое что \eqref{уменьшение f} выполнено.

Алгоритм метода:
\begin{enumerate}
    \item Выбрать \(\varepsilon > 0, \alpha > 0, x\in E_n\), вычислить \(f(x)\).
    \item Вычислить \(\nabla f(x)\). Проверить условие \(||\nabla f(x)|| < \varepsilon\). Если оно выполнено, то завершить процесс, иначе перейти к шагу 3.
    \item Найти \(y = x - \alpha \nabla f(x)\) и \(f(y)\). Если \(f(y) < f(x)\), то положить \(x = y, f(x) = f(y)\) и перейти к шагу 2, иначе --- к 4.
    \item Положить \(\alpha = \frac{\alpha}{2}\) и перейти к шагу 3.
\end{enumerate}

\begin{remark}
    В окрестности стационарной точки величина градиента мала, вследствие чего сходимость процесса замедляется. Поэтому в~\eqref{итерационный процесс} иногда полагают
    \[p^k = - \frac{\nabla f(x^k)}{||\nabla f(x^k)||}\]
\end{remark}

\begin{theorem}
    Пусть симметричная матрица \(A\) квадратичной функции \(f(x)\) положительно определена, \(l\) и \(L\) --- наименьшее и наибольшее собственные значения \(A\) (\(0 < l \leq L\)). Тогда при любых \(\alpha \in \left( 0, \frac{2}{L} \right)\) и \(x^0 \in E_n\) \eqref{итерационный процесс} сходится к единственной точке глобального минимума \(x^*\) функции \(f(x)\) линейно:
    \[\rho(x^k, x^*) \leq q^k \rho(x^0, x^*)\]
\end{theorem}
\begin{proof}
    Т.к. \(A\) положительно определена, то \(f(x)\) сильно выпукла. Следовательно точка \(x^*\) существует и единственна. \(\nabla f(x^*) = 0\), тогда:
    \[\nabla f(x^k) = Ax^k + b = Ax^k + b - Ax^* - b = A(x^k - x^*)\]
    \begin{align*}
        ||x^k - x^*|| & = ||x^{k - 1} - \alpha \nabla f(x^{k - 1}) - x^*|| \\
                      & = ||x^{k - 1} - x^* - \alpha A(x^{k - 1} - x^*)||  \\
                      & = ||(E - \alpha A)(x^{k - 1} - x^*)||
    \end{align*}
    \begin{align*}
        ||x^k - x^*|| & \leq ||E - \alpha A|| \cdot ||x^{k - 1} - x^*|| \\
                      & \leq q ||x^{k - 1} - x^*||                      \\
                      & \leq q^k ||x^0 - x^*||
    \end{align*}
\end{proof}

\(q\) --- оценка нормы матрицы через величину её собственных значений: \(||E - \alpha A|| \leq q = \max \{|1 - \alpha l|, |1 - \alpha L|\} \).  Величина \(q\) принимает наименьшее значение при \(q^* = \frac{L - l}{L + l}\) при \(\alpha = \alpha^* = \frac{2}{L + l}\)
\begin{proof}
    Т.к. \(l < L\), то \(1 - \alpha l = -(1 - \alpha L)\). Тогда \(q = 1 - \alpha l = 1 - \frac{2l}{L + l} = \frac{L - l}{L + l}\)
\end{proof}

От соотношения \(l\) и \(L\) существенно зависит число итераций градиентного метода при минимизации выпуклой квадратичной функции.

\begin{example}[\(L = l > 0\)]
    \(f(x) = x_1^2 + x_2^2 \to \min, x^0 = \begin{pmatrix} 1 & 1 \end{pmatrix}^T, \alpha = \alpha^*\)

    Решение:
    \[A = \begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix} \Rightarrow l = L = 2 \Rightarrow \alpha^* = \frac{2}{2 + 2} = \frac{1}{2}\]
    \[x^1 = x^0 - \frac{1}{2} \nabla f(x^0) = \begin{pmatrix} 0 & 0 \end{pmatrix}^T\]

    Несложно заметить, что \(x^1 = x^*\).

    Таким образом, точка минимума нашлась за один шаг.
\end{example}

При \(l = L\) линии уровня \(f(x)\) --- концентрические окружности. При \(L \gg l > 0\) линии уровня \(f(x)\) --- эллипсы:

\begin{example}[\(L \gg l > 0\)]
    \(f(x) = x_1^2 + 100x_2^2 \to \min , x_0 = \begin{pmatrix} 1 & 1 \end{pmatrix}^T, \alpha = \alpha^*\)

    \[A = \begin{pmatrix} 2 & 0 \\ 0 & 200 \end{pmatrix} \Rightarrow l = 2, L = 200\]
    \[ - \nabla f(x^0) = \begin{pmatrix} - 2 & - 200 \end{pmatrix}^T\]
    \(x^* - x^0 = \begin{pmatrix} - 1 & - 1 \end{pmatrix}^T\) --- направление к точке глобального минимума, сильно отличается от направления спуска, минимизирующая последовательность сходится зигзагообразно.
\end{example}

\begin{definition}
    \textbf{Число обусловленности} для симметричной положительно определенной матрицы: \(\mu = \frac{L}{l}\). Оно характеризует степень вытянутости линий уровня \(f(x) = C\).
\end{definition}

\begin{itemize}
    \item Если \(\mu\) велико, то линии уровня сильно вытянуты, функция имеет овражный характер, т.е. резко меняется по одним направлениям и слабо по другим. В таком случае задачу минимизации называют плохо обусловленной.
    \item Если \(\mu \sim 1\), линии уровня близки к окружности и задача называется хорошо обусловленной.
\end{itemize}

\subsection{Метод наискорейшего спуска}

Идея: после вычисления в начальной точке градиента функции делает в направлении антиградиента не малый шаг, а передвигается до тех пор, пока функция убывает. Достигнув точки минимума на выбранном направлении, повторяет описанную процедуру.

\(\alpha_k\) находится из решения задачи одномерной оптимизации:
\begin{equation}
    \Phi_k(\alpha) \to \min , \Phi_k(\alpha) = f(x^k - \alpha \nabla f(x^k)), \alpha > 0
    \label{задача оптимизации}
\end{equation}

Алгоритм метода:
\begin{enumerate}
    \item Выбрать \(\varepsilon > 0, x^0 \in E_n\), вычислить \(f(x^0)\)
    \item Вычислить \(\nabla f(x)\). Проверить условие \(||\nabla f(x)|| < \varepsilon\). Если оно выполнено, то завершить процесс, иначе перейти к шагу 3.
    \item Решить задачу~\eqref{задача оптимизации} для \(x^k = x\), т.е. найти \(\alpha^*\). Положить \(x = x - \alpha^* \nabla f(x)\), перейти к шагу 2.
\end{enumerate}

\begin{definition}
    Ненулевые вектора \(p^1 \dots p^k\) называются \textbf{сопряженными} относительно матрицы \(A\) размера \(n \times n\) или \textbf{\(A\)-ортогональными}, если \(\ev{A p^i, p^j} = 0\), если \(i \neq j\).
\end{definition}

Система из \(n\) векторов \(p^1 \dots p^n\), сопряженных относительно положительно определенной матрицы \(A\), линейно независима и образует базис в \(E_n\).

Рассмотрим минимизацию квадратичной функции \(f(x) = \frac{1}{2} \ev{Ax, x} + \ev{b, x} + c\) в \(E_n\), где \(A\) положительно определенная и итерационный процесс \eqref{итерационный процесс}, где \(p^k\) --- \(A\)-ортогональные.

Если в таком итерационном процессе на каждом шаге исчерпывающий спуск, то:
\[\alpha_k = -\frac{\ev{\nabla f(x^0), p^k}}{\ev{A p^k, p^k}} \]

\begin{proof}
    \[x^k = x^{k - 1} + \alpha_k p^k = x^0 + \sum_{i = 1}^n \alpha_i p^i\]
    \[\nabla f(x) = Ax + b\]
    \[\nabla f(x^k) = \nabla f(x^0) + \sum_{i = 1}^k \alpha_i A p^i\]
    Домножим на \(p^k\):
    \[\ev{\nabla f(x^k)} = \ev{\nabla f(x^0), p^k} + \ev{\alpha_k A p^k, p^k} \]
    \[\ev{\nabla f(x^0), p^k} + \ev{\alpha_k A p^k, p^k} = 0\]
    Т.к. \(A\) положительно определено, \(\ev{A p^k, p^k} > 0\) и для \(\alpha_k\):
    \[\alpha_k = -\frac{\ev{\nabla f(x^0), p^k}}{\ev{A p^k, p^k}} \]
\end{proof}

\begin{theorem}
    Последовательный исчерпывающий спуск по \(A\)-ортогональным направлениям приводит квадратичной формы не более чем за \(n\) шагов.
\end{theorem}
