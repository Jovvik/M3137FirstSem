\chapter{24 марта}

\subsection{Метод сопряженных градиентов}

\begin{equation}
    p^{k + 1} = - \nabla f (x^{k + 1}) + \beta_k p^k
    \label{процесс сопряженных}
\end{equation}
\(\beta_k\) выбираются так, чтобы получалась последовательность \(A\)-ортогональных векторов \(p^0, p^1 \dots \). Из условия \(\ev{A p^{k + 1}, p^k} = 0\) имеем:
\begin{equation}
    \beta_k = \frac{\ev{A \nabla f(x^{k + 1}, p^k)}}{\ev{A p^k, p^k}}
    \label{beta}
\end{equation}

Для квадратичной функции:
\begin{equation}
    \alpha_k = -\frac{\ev{\nabla f(x^k), p^k}}{\ev{A p^k, p^k}}
    \label{alpha}
\end{equation}

Вышеуказанный итерационный процесс дает точки \(x^0 \dots x^k\) и векторы \(p^0 \dots p^k\), такие, что если \(\nabla f(x^i) \neq 0\) при \(0 \leq i < k \leq n - 1\), то векторы \(p^0 \dots p^k\) \(A\)-ортогональны, а \(\nabla f(x^0) \dots \nabla f(x^i)\) взаимно ортогональны.

Т.к. в~\eqref{процесс сопряженных} \(p^k\) \(A\)-ортогональны, то метод гарантирует нахождение точки минимума сильно выпуклой функции \underline{не более, чем за \(n\) шагов}.

Следующие формулы описывают итерационный процесс метода сопряженных градиентов:
\begin{equation}
    x^{k + 1} = x^k + \alpha_k p^k \quad x^0 \in E_n, p^0 = -\nabla f(x^0)
\end{equation}
\begin{equation}
    f(x^k + \alpha_k p^k) = \min_{\alpha > 0} f(x^k + \alpha p^k)
\end{equation}
\begin{equation}
    p^{k + 1} = - \nabla f(x^{k + 1} + \beta_k p^k)
\end{equation}
\begin{equation}
    \beta_k = \frac{||\nabla f(x^{k + 1})||^2}{||\nabla f(x^k)||^2}
\end{equation}
Можем заметить, что мы не используем матрицу \(A\), поэтому этот метод может применяться для минимизации не только квадратичных функций. Но этот метод может не находить точку минимума не квадратичной функции за конечное число шагов.

Вектора \(p^k\) вообще говоря могут не образовывать \(A\)-ортогональную систему, вследствие чего реализация этого метода будет сопровождаться неизбежными накапливающимся погрешности, из-за чего сходимость метода может нарушиться. Чтобы с этим бороться, через каждые \(N\) шагов производят обновление метода, т.е. \(\beta_{m \cdot N} = 0, m\in\N\), где \(m \cdot N\) называются моментами обновления метода \textit{(рестарта)}, а \(N\) обычно принимают за \(n\) --- размерность пространства \(E_n\).

\subsection{Метод стохастического градиентного спуска}

Этот метод используется, когда дано множество пар \((x,y)\), называемых тренировочными наборами. Это множество разделяется на \(K\) подмножеств размера \(M\), называемых minibatch.\footnote{Проще говоря, этот метод используется в машинном обученнии}

\[X^{(k)} = \{x_i\ |\ i = M_k, \dots (M_k + M - 1)\}\]
\[Y^{(k)} = \{y_i\ |\ i = M_k, \dots (M_k + M - 1)\}\]
\[L^{(k)}(w) = \sum_{i = 0}^M L(w, x_{M_k + i}, y_{M_k + i})\]


Есть большие итерации по \(p\), называемые \textbf{эпохами} и малые итерации \(w_p^{(k + 1)} = w_p^{(k)} - \eta \cdot \nabla L^{(k)}(w_p^{(k)}), w_{p + 1}^{(0)} = w_p^{(K)}\), где \(\eta = \const\)\footnote{И называется learning rate}. При переходе от одной эпохе к другой minibatch-и случайно перемешиваются.

\subsubsection{Adagrad}

\begin{remark}
    Куда более адекватная статья по adagrad и прочим модификациям стохастического спуска: \href{https://habr.com/ru/post/318970/}{https://habr.com/ru/post/318970/}
\end{remark}

\begin{remark}
    Лучшая модификация SGD --- Adam, если не хочется думать, надо всегда использовать его.
\end{remark}

Идея алгоритма --- в покоординатном изменении \(\eta\). Пусть \(\eta_p = \begin{pmatrix} \eta_p^{(1)} & \dots & \eta_p^{(d)} \end{pmatrix} \), \(\eta_0\) --- константный вектор \(\eta_0^{(i)} = \eta \ \ \forall i\).

Вспомогательные данные:
\[\nabla L(w_p) = \begin{pmatrix} g_p^{(1)} & \dots & g_p^{(d)} \end{pmatrix} \quad G_p^{(i)} = \sum_{j = 1}^p (g_j^{(i)})^2\]

Тогда пусть
\[\eta_p^{(i)} = \frac{\eta}{\sqrt{G^{(i)}_p + \epsilon}}\]
, где \(\epsilon \approx 10^{ - 8}\) --- сглаживающий параметр, который позволяет избежать деления на \(0\).

Тогда правило перехода будет:
\[w_{p + 1} = w_p - \eta_p \odot \nabla L(w_p)\]
, где \(\odot\) --- поэлементное умножение векторов.

\subsection{Метод покоординатного спуска}

Алгоритм метода:
\begin{enumerate}
    \item Фиксируем значения всех переменных вектора \(x = \begin{pmatrix} x_1 & \dots & x_n \end{pmatrix} \), кроме \(x_i\).
    \item \(f(x_i) \to \min\) методом одномерной оптимизации \textit{(наиболее популярный метод --- золотого сечения)}.
    \item Проверка критерия остановки:
          \begin{itemize}
              \item \(||x^{k + 1} - x^k|| \leq \varepsilon_1\)
              \item \(||f(x^{k + 1}) - f(x^k)|| \leq \varepsilon_2\)
          \end{itemize}
\end{enumerate}
