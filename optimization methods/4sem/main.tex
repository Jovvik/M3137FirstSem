\input{../../main_preamble.sty}

\begin{document}

\title{Методы оптимизации}
\maketitle

\tableofcontents

\chapter{10 февраля}

Этот курс --- о минимизации \textit{(максимизации)} функционалов. Кроме конкретных методов оптимизации, планируется рассмотреть форматы хранения матриц, о методах работы с ними и рассмотреть 1-2 \textit{(может быть 3)} СЛАУ с использованием различных форматов.

Т.к. значения, получаемые компьютерами --- не точные, нам требуется теория погрешности.

\section{Теория погрешности}

Все погрешности разделяются на два класса:

\begin{enumerate}
    \item Неустранимая --- обусловлена неточностью исходных данных. Например, неточное знание физических констант или других параметров задачи. Тем не менее, необходимо знать эту погрешность, чтобы ставить рамки погрешности для решения.
    \item Устранимая --- погрешность процесса решения задачи. Эту погрешность можно уменьшить выбором метода решения задачи.
          \begin{enumerate}
              \item Погрешность модели
              \item Остаточная погрешность \textit{(погрешность аппроксимации)}

                    Например, аппроксимация ряда первыми \(n\) его членами или аппроксимация по теореме Вейерштрасса квадратичной функцией.

              \item Погрешность округления \label{округления}
              \item Накапливаемая погрешность \label{накапливаемая}
          \end{enumerate}

          \ref{округления} и \ref{накапливаемая} часто объединяют в вычислительную погрешность.
\end{enumerate}

\begin{definition}
    Пусть \(X^*\) --- точное решение, а \(X\) --- найденное \textit{(приближенное)} решение. Тогда \(X^* - X\) называется \textbf{погрешностью}, а её модуль \(\Delta X = |X^* - X|\) --- \textbf{абсолютная погрешность}.
\end{definition}

Разумеется, \(\Delta X\) представляет сугубо теоретический интерес, т.к. \(X^*\) неизвестна и \(\Delta X\) нельзя вычислить.

\begin{definition}
    В качестве требования к решению часто предоставляется \textbf{предельная абсолютная погрешность} \(\Delta_X \geq |X^* - X|\).
\end{definition}

\begin{definition}
    Также существует \textbf{относительная погрешность} \(\delta X = \left|\cfrac{X^* - X}{|X|}\right|\)
\end{definition}

Относительная погрешность позволяет выражать погрешность относительно значений самой величины. Например, при измерении длины парты погрешность 1 см не очень хорошо, а при измерении расстояния между городами --- приемлемо.

\begin{definition}
    \textbf{Предельная относительная погрешность} \(\delta_X \geq \left|\cfrac{X^* - X}{|X|}\right|\)
\end{definition}

\begin{definition}
    \textbf{Значащие цифры} некоторого числа --- все цифры в его изображении, отличные от нуля, а также нули, если они содержатся между значащими цифрами или расположены в конце числа и указывают на сохранение разряда точности.
\end{definition}

\begin{definition}
    Если значащая цифра приближенного значения \(a\), находящаяся в разряде, в котором выполняется условие \(\Delta \leq 0.5 \cdot 10^k\), т.е. абсолютное значение погрешности не превосходит половину единицы этого разряда \textit{(\(k\) --- номер этого разряда)}, то такая цифра называется \textbf{верной в узком смысле}.

    Цифра называется \textbf{верной в широком смысле}, если в определении выше используется \(1\) вместо \(0.5\).
\end{definition}

\begin{example}
    \(a = 3.635, \Delta a = 0.003\)
    \begin{itemize}
        \item \(k = 0 \quad \frac{1}{2} \cdot 10^0 = \frac{1}{2} \geq \Delta a\)
        \item \(k = - 1 \quad \frac{1}{2} \cdot 10^{ - 1} = 0.05 \geq \Delta a\)
        \item \(k = - 2 \quad \frac{1}{2} \cdot 10^{ - 2} = 0.005 \geq \Delta a\)
        \item \(k = - 3 \quad \frac{1}{2} \cdot 10^{ - 3} = 0.0005 < \Delta a\)
    \end{itemize}

    Таким образом, цифра \(5\) является сомнительной, остальные --- верные.
\end{example}

\begin{example}
    Рассмотрим следующие способы записи одного и того же выражения:
    \[\left( \frac{\sqrt{2} - 1}{\sqrt{2} + 1}  \right)^3 = (\sqrt{2} - 1)^6 = (3 - 2\sqrt{2})^3 = 99 - 70\sqrt{2}\]

    Посчитаем все выражения с различными приближениями \(\sqrt{2}\):

    \begin{itemize}
        \item \(\frac{7}{5} = 1.4\)
        \item \(\frac{17}{12} = 1.41666\)
        \item \(\frac{707}{500} = 1.414\)
        \item \(\sqrt{2} = 1.4142135624\)
    \end{itemize}

    \begin{center}\bgroup\def\arraystretch{1.5}
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            \(\sqrt{2}\)        & \(\left( \frac{\sqrt{2} - 1}{\sqrt{2} + 1}  \right)^3\)     & \((\sqrt{2} - 1)^6\)                                                      & \((3 - 2\sqrt{2})^3\)                                    & \(99 - 70\sqrt{2}\)           \\ \hline
            \(\frac{7}{5}\)     & \(\frac{1}{216}\approx 0.00 \underline 4 6\)                & \(\frac{64}{15625}\approx 0.00\underline 51\)                             & \(\frac{1}{125} = 0.008\)                                & \(1\)                         \\ \hline
            \(\frac{17}{12}\)   & \(\frac{125}{24389}\approx 0.00\underline{51}3\)            & \(\frac{15625}{2985354}\approx 0.00\underline52\)                         & \(\frac{1}{216}\approx 0.00\underline46\)                & \( - \frac{1}{6} = - 0.6(6)\) \\ \hline
            \(\frac{707}{500}\) & \(\frac{8869743}{1758416743} \approx 0.00\underline{50}44\) & \(\frac{78672340886049}{15625\cdot 10^{12}} \approx 0.00\underline{50}4\) & \(\frac{636056}{125000000} \approx 0.00\underline{50}9\) & \(0.02\)                      \\ \hline
        \end{tabular}
        \egroup
    \end{center}
\end{example}

\[\Delta_{(X \pm Y)} = \Delta_X + \Delta_Y\]
\[\Delta_{(X\cdot Y)} \approx |Y|\Delta_X + |X|\Delta_Y\]
\[\Delta_{(X / Y)} \approx \left|\frac{1}{Y}\right| \Delta_X + \left|\frac{X}{Y^2}\right| \Delta_Y\]
\[|\Delta u| = |f(x_1 + \Delta x_1, \dots , x_n + \Delta x_n) - f(x_1 \dots x_n)|\]
\[|\Delta u| \approx |df(x_1 \dots x_n)| = \left|\sum_{i = 1}^n \frac{\partial u}{\partial x_i} \Delta x_i\right| \leq \sum_{i = 1}^n \left|\frac{\partial u}{\partial x_i}\right| |\Delta x_i|\]
\[\Delta_u = \sum_{i = 1}^n \left|\frac{\partial u}{\partial x_i}\right| \Delta x_i\]
\[|\delta u| = \sum_{i = 1}^n \left|\frac{\partial \ln u}{\partial x_i}\right||\Delta x_i|\]
\[\delta_u = \sum_{i = 1}^n \left|\frac{\partial \ln u}{\partial x_i}\right||\Delta x_i|\]
\[\delta_{(X \pm Y)} = \left|\frac{X}{X \pm Y}\right|\delta_X + \left|\frac{Y}{X \pm Y}\right|\delta_Y\]
\[\delta_{(X\cdot Y)} = \delta_X + \delta_Y\]
\[\delta_{(X / Y)} = \delta_X + \delta_Y\]

Вернемся к прошлому примеру и посчитаем относительную погрешность.

\(\sphericalangle x = \frac{7}{5}\)

\[\delta_{f_1} = 3 \left|\frac{1}{x - 1} - \frac{1}{x + 1}\right| \cdot |\delta x| = 6.25 |\delta x|\]
\[\delta_{f_2} = 6 \left|\frac{1}{x - 1}\right| \cdot |\delta x| = 15 |\delta x|\]
\[\delta_{f_3} = 6 \left|\frac{1}{3 - 2x}\right| \cdot |\delta x| = 30 |\delta x|\]
\[\delta_{f_4} = \left|\frac{90}{99 - 70x}\right| \cdot |\delta x| = 70 |\delta x|\]

Таким образом, наибольшую погрешность даёт \(f_4\), наименьшую --- \(f_1\).

\begin{example}
    \[y^2 - 140y + 1 = 0\]
    \[y = 70 - \sqrt{4899}\]
    \[\sqrt{4899}\approx 69.99\]
    \[y\approx 70 - 69.99 = 0.0\underline 1\]
    Посчитаем другим методом --- избавимся от вычитания похожих чисел.
    \[y = \frac{1}{70 + \sqrt{4899}}\]
    \[y = \frac{1}{139.99}\approx \frac{1}{140} = 0.00714285 \approx 0.00\underline{7143}\]

    Можно заметить, что результат весьма точнее.
\end{example}

\begin{example}
    Рассмотрим задачу вычисления суммы \(S = \sum_{j = 1}^{10^6} \frac{1}{j^2}\).

    Если суммировать по формуле \(S_n = S_{n - 1} + \frac{1}{n^2}\), то из-за того, что сначала суммируются большие числа, а потом малые, погрешность велика: \(\Delta = 10^6 \cdot 2^{ - 1}\approx 2\cdot 10^{ - 4}\)

    Если же суммировать с конца, то \(\Delta = \mathcal{O}\left( \frac{1}{n} \right) \approx 6\cdot 10^{ - 8}\)
\end{example}

Рекомендации для увеличения точности вычислений:
\begin{enumerate}
    \item Если складывать или вычитать последовательность чисел, то лучше начинать с малых членов. % uwu (поздравляю, вы нашли пасхалку)
    \item Желательно избавляться от вычитания двух почти равных чисел, по возможности преобразую формулу.
    \item Необходимо сводить к минимуму число математических операций. Это также способствует ускорению работы алгоритма.
    \item Если ЯП и компьютер позволяют использовать числа разных типов, то числа с большим числом разрядов всегда повышают точность вычислений \textit{(в ущерб памяти)}. % спасибо, кэп
\end{enumerate}

Дробные числа нужно сравнивать с помощью \(\varepsilon\), т.е. \(|a - b| \leq \varepsilon\)

\section{Задачи оптимизации. Вводное.}

Здесь и далее \textbf{целевая функция} --- функция, которую мы минимизируем.

\begin{obozn}
    Пусть целевая функция --- \(f(x)\). Это обозначается как \(f(x) \xrightarrow{x\in U} \min\).

    \(f(x) \to \max \Rightarrow - f(x) \to \min\). Таким образом, мы без потери общности рассматриваем задачу минимизации.
\end{obozn}

\begin{definition}
    Если \(\exists x^* \in U \ \ f(x^*) \leq f(x) \ \ \forall x\in U\), то такой \(x^*\) называется \textbf{точкой \textit{(глобального)} минимума}
\end{definition}

\begin{obozn}
    Множество всех точек минимума обозначается \(U^* = \{x^*_i\ |\ i = 1\dots k\} \)
\end{obozn}

Мы рассматриваем класс функций таких, что \(U^* \neq \emptyset\)

\begin{definition}
    Функция \(f(x)\) называется \textbf{унимодальной} на \([a, b]\), если она:
    \begin{enumerate}
        \item Непрерывна на \([a, b]\)
        \item \(\exists \alpha, \beta : a \leq \alpha \leq \beta \leq b\), такие что:
              \begin{enumerate}
                  \item Если \(a < \alpha\), то на \([a, \alpha] \quad f(x)\) строго монотонно убывает.
                  \item Если \(\beta < b\), то на \([\beta, b] \quad f(x)\) строго монотонно возрастает.
                  \item \(\forall x\in [\alpha, \beta] \ \ f(x) = f_* = \min\limits_{[a, b]} f(x)\)
              \end{enumerate}
    \end{enumerate}
\end{definition}

\begin{figure}[h]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includesvg[width=.9\linewidth]{images/вырожденная_унимодальная.svg}
        \caption{Вырожденные \(\alpha\) и \(\beta\), унимодальная функция}
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \centering
        \includesvg[width=.9\linewidth]{images/не_вырожденная_унимодальная.svg}
        \caption{Унимодальная функция}
    \end{minipage}
\end{figure}

\begin{prop}\itemfix
    \begin{enumerate}
        \item Если функция унимодальна на \([a, b]\), то она унимодальна и на \([c, d] \subset [a, b]\)
        \item Если \(f\) унимодальна на \([a, b], a \leq x_1 < x_2 \leq b\), тогда:
              \begin{enumerate}
                  \item Если \(f(x_1) \leq f(x_2)\), то \(x^*\in [a, x_2]\)
                  \item Если \(f(x_1) > f(x_2)\), то \(x^*\in[x_1, b]\)
              \end{enumerate}
    \end{enumerate}
\end{prop}

\begin{definition}
    \(f(x)\), заданная на \([a, b]\), называется выпуклой на этом отрезке, если
    \[\forall x', x''\in [a, b], \alpha\in[0,1] \quad f(\alpha x' + (1 - \alpha)x'') \leq \alpha f(x') + (1 - \alpha)f(x'')\]
\end{definition}

\begin{prop}\itemfix
    \begin{enumerate}
        \item Если \(f(x)\) выпукло на \([a, b]\), то \(\forall [x', x''] \subset [a, b]\), то её график расположен ниже хорды между \(x'\) и \(x''\)
        \item Всякая выпуклая функция на отрезке является унимодальной на нём.
    \end{enumerate}
\end{prop}

\begin{definition}
    \textbf{Стационарные точки} --- точки \(x\), для которых \(f'(x) = 0\).
\end{definition}

Мы будем рассматривать одномерные задачи оптимизации, т.к. многомерные задачи часто сводятся к одномерным.

\section{Одномерная минимизация функций. Прямые методы.}

Прямые методы --- методы, не использующие производные целевой функции.

\subsection{Метод дихотомии}

Этот метод --- тернарный поиск.

\[x_1 = \frac{b + a - \delta}{2} \quad x_2 = \frac{b + a + \delta}{2}\]
\[\tau = \frac{b - x_1}{b - a} = \frac{x_2 - a}{b - a} \to \frac{1}{2}\]
\[x^* \in [a_i, b_i] \ \ \forall i\]

\begin{itemize}
    \item [Шаг 1:] Находим \(x_1\) и \(x_2\), вычисляем \(f(x_1)\) и \(f(x_2)\)
    \item [Шаг 2:] Сравниваем \(f(x_1)\) и \(f(x_2)\).
          \begin{itemize}
              \item Если \(f(x_1) \leq f(x_2)\), переходим к отрезку \([a, x_2]\), т.е. \(b = x_2\)
              \item Иначе переходим к \([x_1, b]\), т.е. \(a = x_1\)
          \end{itemize}
    \item [Шаг 3:] \(\varepsilon_n = \frac{b - a}{2} \), где \(n\) --- номер итерации.
          \begin{itemize}
              \item Если \(\varepsilon_n > \varepsilon\), переходим к новой итерации.
              \item Если \(\varepsilon_n \leq \varepsilon\), завершаем поиск и переходим к шагу 4.
          \end{itemize}
    \item [Шаг 4:] \(X^* \approx \overline X = \frac{a + b}{2}\)
\end{itemize}

\begin{remark}
    \(\delta\) выбирается на интервале \((0, 2\varepsilon)\). Чем меньше \(\delta\), тем больше относительное уменьшение длины отрезка на каждой итерации. При черезмерно малом \(\delta\) сравнение \(f(x_1)\) и \(f(x_2)\) будет затруднительно, т.к. они близки.
\end{remark}

Мы можем оценить число необходимых итераций:
\[n \geq \log_2 \frac{b - a - \delta}{2\varepsilon - \delta}\]

\chapter{17 февраля}

\subsection{Метод золотого сечения}

Рассмотрим отрезок \([0, 1]\). Пусть \(x_2 = \tau\), тогда симметрично расположенная \(x_1 = 1 - \tau\). Пусть дальше был выбран отрезок \([0, \tau]\), тогда пусть \(x_2' = 1 - \tau\). Чтобы новые точки делили отрезок в таком же соотношении, необходимо, чтобы \(\frac{1}{\tau} = \frac{\tau}{1 - \tau} \Rightarrow \tau^2 = 1 - \tau \Rightarrow \tau = \frac{\sqrt{5} - 1}{2} \approx 0.61803\). Таким образом, \(x_1 = 1 - \tau = \frac{3 - \sqrt{5}}{2}, x_2 = \tau = \frac{\sqrt{5} - 1}{2}\)

В общем случае для отрезка \([a, b]\):
\begin{equation}
    x_1 = a + \frac{3 - \sqrt{5}}{2}(b - a), x_2 = a + \frac{\sqrt{5} - 1}{2} (b - a) \label{x_1, x_2, метод золотого сечения}
\end{equation}

Вычислим погрешность:
\[\Delta_n = \tau^n (b - a) \quad \varepsilon_n = \frac{\Delta_n}{2} = \frac{1}{2}\left( \frac{\sqrt{5} - 1}{2} \right)^n (b - a)\]

Для заданного \(\varepsilon\) условия окончания \(\varepsilon_n \leq \varepsilon\).

Результат метода:
\[x^* = \frac{a_{(n)} + b_{(n)}}{2}\]

Оценка числа шагов для достижения искомой точности:
\[n \geq \ln\left( \frac{\frac{2\varepsilon}{b - a}}{\ln \tau} \right) \approx 2 \cdot 1 \cdot \ln\left( \frac{b - a}{2\varepsilon} \right)\]

\begin{itemize}
    \item [Шаг 1:] Находим \(x_1\) и \(x_2\) по формуле \eqref{x_1, x_2, метод золотого сечения}, вычисляем \(f(x_1)\) и \(f(x_2)\). \(\varepsilon_n = \frac{b - a}{2}, \tau = \frac{\sqrt{5} - 1}{2} \).
    \item [Шаг 2:]
          \begin{itemize}
              \item Если \(\varepsilon_n > \varepsilon\), переходим к шагу 3.
              \item Если \(\varepsilon_n \leq \varepsilon\), переходим к шагу 4.
          \end{itemize}
    \item [Шаг 3:] Сравниваем \(f(x_1)\) и \(f(x_2)\).
          \begin{itemize}
              \item Если \(f(x_1) \leq f(x_2)\), то \(b = x_2, x_2 = x_1, x_1 = b - \tau (b - a)\). Мы запоминаем \(f(x_2)\) для следующего шага, т.к. оно равно \(f(x_1)\) на этом шаге.
              \item Иначе \(a = x_1, x_1 = x_2, f(x_1) = f(x_2)\). Мы запоминаем \(f(x_1)\) для следующего шага, т.к. оно равно \(f(x_2)\) на этом шаге.
          \end{itemize}
    \item [Шаг 4:] \(X^* \approx \overline X = \frac{a_{(n)} + b_{(n)}}{2}\)
\end{itemize}

\subsection{Метод Фибоначчи}

Мы знаем, что \(F_n = \frac{\left( \frac{1 + \sqrt{5}}{2} \right)^n - \left( \frac{1 - \sqrt{5}}{2} \right)^n}{\sqrt{5}} \), а также при \(n \to +\infty\ \) \(F_n \approx \frac{\left( \frac{1 + \sqrt{5}}{2} \right)^n}{\sqrt{5}}\)

Рассмотрим нулевую итерацию:
\[x_1 = a + \frac{F_n}{F_{n + 2}} (b - a) \quad x_2 = a + \frac{F_{n+1}}{F_{n + 2}} (b - a)\]

Рассмотрим \(k\)-тую итерацию:
\[x_1 = a_{(k)} + \frac{F_{n - k + 1}}{F_{n - k + 3}} (b_k - a_k) = a_k + \frac{F_{n - k + 1}}{F_{n + 2}} (b_0 - a_0)\]
\[x_2 = a_{(k)} + \frac{F_{n - k + 2}}{F_{n - k + 3}} (b_k - a_k) = a_k + \frac{F_{n - k + 2}}{F_{n + 2}} (b_0 - a_0)\]

Пусть \(k = n\), тогда:
\[x_1 = a_n + \frac{F_1}{F_{n + 2}} (b_0 - a_0) \quad x_2 = a_n + \frac{F_2}{F_{n + 2}} (b_0 - a_0)\]

Условие на погрешность:
\[\frac{b_n - a_n}{2} = \frac{b_0 - a_0}{F_{n + 2}} < \varepsilon\]
Какое брать \(n?\) Такое, что \(\frac{b_0 - a_0}{\varepsilon} < F_{n + 2}\)

Есть проблема, при большом \(n\) \(\frac{F_n}{F_{n + 2}}\) есть бесконечная десятичная дробь, вследствие чего образуется погрешность.

\subsection{Метод парабол}

\begin{figure}[h]
    \centering
    \includesvg{images/метод_параболы.svg}
    \caption{Функция \(f(x)\) и её приближение параболой.}
\end{figure}

Пусть \(\exists x_1, x_2, x_3\in[a, b]\), такие что \(\begin{cases}
    x_1 < x_2 < x_3 \\
    f(x_1) \geq f(x_2) \leq f(x_3)
\end{cases}\)

Тогда приближающая парабола имеет вид \(q(x) = a_0 + a_1(x - x_1) + a_2(x - x_1)(x - x_2)\). Мы имеем условия на коэффициенты этой параболы: \(\begin{cases}
    q(x_1) = f(x_1) = f_1 \\
    q(x_2) = f(x_2) = f_2 \\
    q(x_3) = f(x_3) = f_3
\end{cases}\)

Коэффициенты можно найти следующим образом:
\[a_0 = f_1 \quad a_1 = \frac{f_2 - f_1}{x_2 - x_1} \quad a_2 = \frac{1}{x_3 - x_2} \left( \frac{f_3 - f_1}{x_3 - x_1} - \frac{f_2 - f_1}{x_2 - x_1} \right) \]

Тогда результат итерации есть \(\overline x = \frac{1}{2} \left( x_1 + x_2 - \frac{a_1}{a_2} \right)\), на следующей лекции будет рассказан переход к следующей итерации.

Точки \(x_1, x_2, x_3\) для новой итерации выбираются следующим образом:
\begin{enumerate}
    \item \begin{enumerate}
              \item Если \(x_1 < \overline x < x_2 < x_3\) и \(f(\overline x) \geq f(x_2)\), то \(x^* \in [\overline x, x_3], x_1 = \overline x\), точки \(x_2\) и \(x_3\) не меняются.
              \item Если \(x_1 < \overline x < x_2 < x_3\) и \(f(\overline x) < f(x_2)\), то \(x^* \in [x_1, x_2], x_3 = x_2, x_2 = \overline x\), точка \(x_1\) не меняется.
          \end{enumerate}
    \item \begin{enumerate}
              \item Если \(x_1 < x_2 < \overline x < x_3\) и \(f(\overline x) \leq f(x_2)\), то \(x^* \in [x_2, x_3], x_1 = x_2, x_2 = \overline x\), точка \(x_3\) не меняется.
              \item Если \(x_1 < x_2 < \overline x < x_3\) и \(f(\overline x) > f(x_2)\), то \(x^* \in [x_1, \overline x], x_3 = \overline x\), точки \(x_1\) и \(x_2\) не меняются.
          \end{enumerate}
\end{enumerate}

\begin{remark}
    Метод парабол имеет квадратичную сходимость.
\end{remark}

\begin{remark}
    Метод парабол требует гладкость функции, что неверно для предыдущих методов.
\end{remark}

\subsection{Комбинированный метод Брента}

Для собственного изучения.

\chapter{24 февраля}

\subsection{Метод равномерного перебора}

\begin{itemize}
    \item [Шаг 1:] Если \(f(x_0) > f(x_0 + \delta)\), то \(k = 1, x_1 = x_0 + \delta, h = \delta\)

          иначе \(x_1 = x_0, h = - \delta\)

    \item [Шаг 2:] \(h = 2h, x_{k+1} = x_k + h\)
    \item [Шаг 3:] Если \(f(x_k) > f(x_{k+1})\), то \(k = k + 1\) и переходим к шагу 2. Иначе прекращаем поиск и искомое лежит в \([x_{k - 1}, x_{k + 1}]\)
\end{itemize}

\section{Методы оптимизации, использующие производную}

В рамках этой главы \(f(x)\) --- дифференцируемая или дважды дифференцируемая выпуклая функция.

Есть три классических метода, использующих производную:
\begin{itemize}
    \item Средней точки
    \item Метод хорд
    \item Метод Ньютона
\end{itemize}

\(f'(x) = 0\) --- необходимое и достаточное условие глобального минимума. Таким образом, условие остановки вычислений --- \(f'(x) \approx 0\), т.е. \(|f'(x)| \leq \varepsilon\)

\subsection{Методы средней точки}

Средняя точка \(\overline x = \frac{a + b}{2}\).

Общая идея алгоритма:
\begin{itemize}
    \item Если \(f'(x) > 0\), то \(\overline x\in\) участку монотонного возрастания \(f(x)\) и \(x^* < \overline x\), т.е. минимум лежит на \([a, \overline x]\)
    \item Если \(f'(x) < 0\), то аналогично можем вывести, что минимум лежит на \([\overline x, b]\)
    \item Если \(f'(x) = 0\), то мы нашли решение.
\end{itemize}

Перепишем это в виде алгоритма:

\begin{itemize}
    \item [Шаг 1:] \(\overline x = \frac{a + b}{2}\), вычислим \(f'(\overline x)\)
    \item [Шаг 2:] Если \(|f'(x)| \leq \varepsilon\), то \(x^* = \overline x\) и завершаем вычисление.
    \item [Шаг 3:] Сравниваем \(f'(x)\) с нулём: \begin{itemize}
              \item Если \(f'(x) > 0\), то \(x^*\in[a, \overline x]\) и \(b = \overline x\)
              \item Иначе \(x^*\in[\overline x, b]\) и \(a = \overline x\)
          \end{itemize}
\end{itemize}

Длина отрезка после \(n\) итераций есть \(\Delta_n = \frac{b - a}{2^n}\)

\subsection{Метод хорд \textit{(метод секущей)}}

Если \(\exists f'(x)\) на \([a, b]\), \(f'(a) \cdot f'(b) < 0\) и \(f'(x)\) непрерывна на \([a, b]\), то \(\exists x\in(a, b) : f'(x) = 0\).

\(F(x) = f'(x)\). Пусть \(\tilde{x}\) --- точка пересечения хорды \(F(x)\) с осью \(Ox\) на \([a, b]\)

\begin{figure}[h]
    \centering
    \includesvg[scale=0.9]{images/хорда.svg}
\end{figure}

Можем тривиально вывести \(\tilde{x}\) из уравнения прямой по двум точками:
\begin{equation}
    \tilde{x} = a - \frac{f'(a)}{f'(a) - f'(b)}(a - b) \label{пересечение хорды}
\end{equation}

\begin{itemize}
    \item [Шаг 1:] Считаем \(\tilde{x}\) по \eqref{пересечение хорды}
    \item [Шаг 2:] Если \(|f'(\tilde{x})| \leq \varepsilon\), то \(x^* = \tilde{x}\) и мы заканчиваем вычисление.

          Иначе шаг 3.
    \item [Шаг 3:] Переходим к новому отрезку:
          \begin{itemize}
              \item Если \(f'(\tilde{x}) > 0\), то \(x^* \in [a, \tilde{x}], b = \tilde{x}, f'(b) = f'(\tilde{x})\), переходим к шагу 1
              \item иначе \(x^* \in [\tilde{x}, b], a = \tilde{x}, f'(a) = f'(\tilde{x})\), переходим к шагу 1
          \end{itemize}
\end{itemize}

\begin{remark}
    Если \(f'(a) \cdot f'(b) \geq 0\), то \(x^* = a\) или \(x^* = b\).
\end{remark}

\subsection{Метод Ньютона \textit{(метод касательной)}}

Если \(f\) выпуклая на \([a, b]\) и дважды непрерывно дифференцируемая, то уравнение \(f'(x) = 0\) решается методом Ньютона.

Пусть \(x_0 \in [a, b]\) --- начальное приближение \(x^*\). \(F(x) = f'(x)\) линеаризуема в окрестности \(x_0\), т.е.
\begin{equation}
    F(x) \approx F(x_0) + F'(x_0)(x - x_0)
\end{equation}

Пусть \(x_1\) --- следующее приближение к \(x^*\). Это будет пересечение касательной с \(Ox\). Найдём эту точку.

\begin{align*}
    F(x_0) + F'(x_0) (x_1 - x_0) & = 0                            \\
    x_1                          & = x_0 - \frac{F(x_0)}{F'(x_0)}
\end{align*}

Таким образом, мы можем получить \(\{x_k\}_{k = 1}^{n}\) --- итерационную последовательность.

\[x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)} \]

Условие остановки такое же, как в предыдущих методах: \(|f'(x_k)| \leq \varepsilon\)

\chapter{3 марта}

Пусть \(x_k\) --- текущая оценка решения \(x^*\)

Рассмотрим ряд Тейлора:
\[f(x_k + p) = f(x_k) + pf'(x_k) + \frac{1}{2}p^2  f''(x_k) + \dots \]
\begin{align*}
    f(x*) & = \min_x f(x)                                                               \\
          & = \min_p f(x_k + p)                                                         \\
          & = \min_p \left( f(x_k) + pf'(x_k) + \frac{1}{2}p^2 f''(x_k) + \dots \right) \\
          & \approx \min_p \left( f(x_k) + pf'(x_k) + \frac{1}{2}p^2 f''(x_k) \right)   \\
\end{align*}

Приравняем производную выражения под \(\min\) к нулю:
\[f'(x_k) + pf''(x_k) = 0\]
\[p = -\frac{f'(x_k)}{f''(x_k)} \]

Тогда \(x^* \approx x_k + p\) и \(x_{k+1} = x_k + p = x_k - \frac{f'(x_k)}{f''(x_k)} \)

Главное преимущество метода Ньютона --- квадратичная скорость сходимости, т.е. если \(x_k\) достаточно близка к \(x^*\) и \(f''(x^*) > 0\), то \(|x_{k+1} - x^*| \leq \beta|x_k - x^*|^2\)

Метод Ньютона может потерпеть неудачу в следующих случаях:
\begin{enumerate}
    \item \(f(x)\) плохо аппроксимируется первыми тремя членами в ряде Тейлора. Тогда \(x_{k+1}\) может быть хуже \textit{(как аппроксимация)} \(x_k\).
    \item \(f''(x_k) = 0\), тогда \(p\) не определен.
    \item Кроме \(f\) нужно вычислять \(f'\) и \(f''\), что затруднительно в реальных задачах.
\end{enumerate}

Мы можем аппроксимировать производную по определению:
\[f'(x_k) \approx \frac{f(x_k + h) - f(x_k)}{h} \]
Эта формула называется правой разностной схемой, у нее есть улучшение, называемое центральной разностной схемой:
\[f'(x_k) \approx \frac{f(x_k + h) - f(x_k - h)}{2h} \]

Если \(f(x)\) --- квадратичная функция, то метод Ньютона сходится за один шаг при любом выборе \(x_0\).

\subsubsection{Достаточное условие монотонной сходимости метода Ньютона}

Пусть \(x^* \in [a, b]\) и \(f(x)\) трижды непрерывно дифференцируемая и выпуклая на \([a, b]\) функция. Тогда \(\{x_k\}\) будет сходиться к пределу \(x^*\) монотонно, если \(0 < \frac{x^* - x_{k+1}}{x^* - x_k} < 1\)

\[f'(x^*) = 0 = f'(x_k) + f''(x_k)(x^* - x_k) + \frac{f'''(x)}{2} (x^* - x_k)^2\]
\[\frac{x^K - x_{k+1}}{x^* - x_k} = \frac{x^* - x_k + \frac{f'(x_k)}{f''(x_k)}}{x^* - x_k} = 1 - \frac{2}{2 + \frac{f'''(x) (x^* - x_k)^2}{f'(x_k)}}\]
Последовательность итераций \(\{x_k\}\) монотонна, если \(\frac{f'''(x)}{f'(x_k)} > 0\), таким образом условие монотонной сходимости метода Ньютона --- постоянство на \(x\in [x^*, x_0]\) знака \(f'''(x)\) и его совпадение с \(f'(x_0)\).

\begin{example}
    \(f(x) = x \cdot \arctg (x) - \frac{1}{2} \?\)

    \[f'(x) = \arctg x \quad f''(x) = \frac{1}{1 + x^2} > 0 \quad f'''(x) = - \frac{2x}{(1 + x^2)^2}\]

    \(f'(x) \cdot f'''(x) < 0\), таким образом не будет монотонной сходимости.

    Пусть \(x_0 = 1\).

    \begin{tabular}{c|c|c|c}
        \(k\) & \(x_k\)              & \(f'(x_k)\)  & \(f''(x_k)\)    \\ \hline
        \(0\) & \(1\)                & \(0.785\)    & \(\frac{1}{2}\) \\
        \(1\) & \( - 0.57\)          & \( - 0.518\) & \(a\)           \\
        \(2\) & \(0.117\)            & \( 0.\)      &                 \\
              &                      &              &                 \\
        \(4\) & \(9\cdot 10^{ - 8}\) &
    \end{tabular}
\end{example}

\subsection{Модификации метода Ньютона}

\subsubsection{Метод Ньютона-Рафсона}

\[x_{k+1} = x_k - \tau_k \frac{f'(x_k)}{f''(x_k)}, 0 < \tau_k \leq 1\]

\(\tau_k\) --- константы. Если \(\tau = 1\), то метод Ньютона-Рафсона вырождается в метод Ньютона.

Для нахождения \(\tau_k\) зададим \(\varphi(\tau)\):

\[\varphi(\tau) = f(x_k - \tau \frac{f'(x_k)}{f''(x_k)} ) \to \min\]

Тогда \[\tau_k = \frac{(f'(x_k))^2}{(f'(x_k))^2 + (f'(\tilde{x}))^2} \text{ , где } \tilde{x} = x_k - \frac{f'(x_k)}{f''(x_k)}  \]

\subsubsection{Метод Марквардта}

\[x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k) + \mu_k} \]
, где \(\mu_k > 0\)

\(\mu_0\) выбирают на порядок выше значения \(f''(x_0)\), \(\mu_{k + 1} = \begin{cases} \frac{m_k}{2} & , \text{ если } f(x_{k+1}) < f(x_k) \\ \mu_{k+1} = 2 \mu_k & , \text{ если } f(x_{k+1}) \geq f(x_k) \end{cases}\)

\section{Метод минимизации многомодальных функций \textit{(метод ломаных)}}

\begin{definition}
    \(f(x), x\in[a, b]\) \textbf{удовлетворяет условию Липшица}, если \(\forall x_1, x_2\in[a, b] \ \ |f(x_1) - f(x_2)| \leq L |x_1 - x_2|\)
\end{definition}

\begin{itemize}
    \item [Шаг 1] Возьмём \(x_1^* = \frac{1}{2L}(f(a) - f(b) + L(a + b))\) и \(p_1^* = \frac{1}{2}(f(a) + f(b) + L(a - b))\). Добавим в рассматриваемое множество \(x_1' = x_1^* - \Delta_1\) и \(x_1'' = x_1^* + \Delta_1\), где \(\Delta_1 = \frac{1}{2L} (f(x_1^*) - p_1)\)
    \item [Шаг 2] Ииз пар \((x_1', p_1)\) и \((x_1'', p_1)\) выберем пару с минимальной \(p\) : \((x_2^*, p_2^*)\) и исключим из рассматриваемого множества.
    \item [Шаг \(n\)] В результате мы получим множество из \(n\) пар \((x, p)\). Исключаем пару с минимальной \(p\) и вместо неё
\end{itemize}

\begin{example}
    \(f(x) = \frac{\sin x}{x} \), \([a, b] = [10, 15]\), \(\varepsilon = 0.01\)

    Проверим условие Липшица:
    \[|f'(x)| = \left|\frac{x\cos x - \sin x}{x^2} \right| < \frac{x |\cos x| + \sin |x|}{x^2} < \frac{x + 1}{x^2} \leq 0.11\]

    \begin{tabular}{c|c|c|c|c|c|c}
        \(n\)  & \(x_n^*\)  & \(p_n^*\)    & \(2L \Delta_n\)         & \(x_n'\)   & \(x_n''\)  & \(p_n\)      \\ \hline
        \(1\)  & \(12.056\) & \( - 0.281\) & \(0.240\)               & \(10.963\) & \(13.149\) & \( - 0.161\) \\
        \(2\)  & \(10.963\) & \( - 0.161\) & \(0.070\)               & \(10.646\) & \(11.280\) & \( - 0.126\) \\
        \(3\)  & \(13.149\) & \( - 0.161\) & \(0.203\)               & \(12.227\) & \(14.701\) & \( - 0.096\) \\
        \(4\)  & \(10.646\) & \( - 0.126\) & \(0.038\)               & \(10.474\) & \(10.818\) & \( - 0.107\) \\
        \(5\)  & \(11.280\) & \( - 0.126\) & \(0.041\)               & \(11.094\) & \(11.466\) & \( - 0.106\) \\
        \(6\)  & \(10.474\) & \( - 0.107\) & \(0.024\)               & \(10.364\) & \(10.584\) & \( - 0.095\) \\
        \(7\)  & \(10.818\) & \( - 0.107\) & \(0.160\)               & \(10.745\) & \(10.891\) & \( - 0.099\) \\
        \(8\)  & \(11.094\) & \( - 0.106\) & \(0.016\)               & \(11.020\) & \(11.168\) & \( - 0.098\) \\
        \(9\)  & \(11.466\) & \( - 0.106\) & \(0.028\)               & \(11.338\) & \(11.594\) & \( - 0.092\) \\
        \(10\) & \(10.891\) & \( - 0.099\) & \(0.008 < \varepsilon\) &            &            &              \\
    \end{tabular}
\end{example}

\chapter{10 марта}

\section{Минимизация функций многих переменных}

\subsection{Постановка задачи}

Необходимо найти \(x^* = \begin{pmatrix} x_1 & x_2 & \dots & x_n \end{pmatrix}^T \in U \subset E_n\), где \(U\) --- множество допустимых значений, а \(E_n\) --- евклидово пространство размера \(n\), при этом \(f(x^*) = \min_{x\in U} f(x)\).

\begin{remark}\itemfix
    \begin{enumerate}
        \item Как и в одномерном случае, задача минимизации эквивалентна задачи максимизации и в общем случае называется задачей поиска экстремума.
        \item Если \(U\) задается ограничениями на вектор \(x\), то такая задача оптимизации называется задачей поиска условного экстремума.
        \item Если \(U = E_n\), т.е. не имеет ограничений, то такая задача оптимизации называется задачей поиска безусловного экстремума.
        \item Решением задачи поиска экстремума называется пара \((x^*, f(x^*))\).
    \end{enumerate}
\end{remark}

\begin{definition}
    Если \(f(x^*) \leq f(x) \ \ \forall x\in U\), то \(x^*\) называется \textbf{глобальным минимумом}.
\end{definition}
\begin{definition}
    Если \(\exists \varepsilon > 0 : ||x - x^*|| < \varepsilon \Rightarrow f(x^*) \geq f(x)\), то \(x^*\) называется \textbf{локальным минимумом}.
\end{definition}
\begin{remark}
    \[||x|| = \sqrt{\sum_i x_i}\]
\end{remark}

\begin{definition}
    \textbf{Поверхностью уровня} функции \(f(x)\) называется множество точек, в которых функция принимает постоянное значение.
\end{definition}

\begin{definition}
    \textbf{Градиентом} \(\nabla f(x)\) непрерывно дифференцируемой функции \(f(x)\) в \(x\) называется:
    \[\nabla f(x) = \begin{pmatrix} \frac{\partial f(x)}{\partial x_1} \\ \frac{\partial f(x)}{\partial x_2} \\ \vdots \\ \frac{\partial f(x)}{\partial x_n} \end{pmatrix} \]
\end{definition}

\begin{remark}
    Градиент направлен по нормали к поверхности уровня, т.е. перпендикулярно к касательной плоскости, проведенной в точке \(x\) в сторону наибольшего возрастания функции.
\end{remark}

\begin{definition}
    Матрица Гессе \(\mathbf H(x)\) дважды непрерывно дифференцируемой в точке \(x\) функции \(f(x)\) называется матрица частных производных производных второго порядка, вычисленных в данной точке.

    \[\mathbf H(x) = \begin{pmatrix}
            \frac{\partial^2 f(x)}{\partial x_1^2}            & \frac{\partial^2 f(x)}{\partial x_1 \partial x_2} & \dots  & \frac{\partial^2 f(x)}{\partial x_1 \partial x_n} \\
            \vdots                                            & \vdots                                            & \ddots & \vdots                                            \\
            \frac{\partial^2 f(x)}{\partial x_n \partial x_1} & \frac{\partial^2 f(x)}{\partial x_n\partial x_2}  & \dots  & \frac{\partial^2 f(x)}{\partial x_n^2}
        \end{pmatrix} = \begin{pmatrix}
            h_{11} & h_{12} & \dots  & h_{1n} \\
            \vdots & \vdots & \ddots & \vdots \\
            h_{n1} & h_{n2} & \dots  & h_{nn}
        \end{pmatrix}  \]
\end{definition}

\begin{enumerate}
    \item \(\mathbf H(x)\) симметрична, имеет размер \(n \times n\).
    \item Можно определить антиградиент --- вектор, равный по модулю градиенту и направленный противоположно. Антиградиент указывает в сторону наибольшего убывания \(f(x)\).
    \item \(\Delta f(x) = f(x + \Delta x) - f(x) = \nabla f(x)^T \Delta x + \frac{1}{2} \Delta x^T \mathbf H(x) \Delta x + \smallO(||\Delta x||^2)\), где \(\smallO(||\Delta x||^2)\) есть сумма всех членов разложения, имеющих порядок выше второго. Можем заметить, что \(\Delta x^T \mathbf H(x) \Delta x\) --- квадратичная форма.
\end{enumerate}

\begin{definition}
    \textbf{Квадратичная форма} \(\Delta x^T \mathbf H(x) \Delta x\)\footnote{и соответствующая ей матрица \(\mathbf H(x)\)} называется:
    \begin{itemize}
        \item Положительно определенной, если \(\forall \Delta x \neq 0 \ \ \Delta x^T \mathbf H(x) \Delta x > 0\)
        \item Отрицательно определенной, если \(\forall \Delta x \neq 0 \ \ \Delta x^T \mathbf H(x) \Delta x < 0\)
        \item Положительно полуопределенной, если \(\forall \Delta x \ \ \Delta x^T \mathbf H(x) \Delta x \geq 0\) и имеется \(\Delta x \neq 0 : \Delta x^T \mathbf H(x) \Delta x = 0\)
        \item Отрицательно полуопределенной, если \(\forall \Delta x \ \ \Delta x^T \mathbf H(x) \Delta x \leq 0\) и имеется \(\Delta x \neq 0 : \Delta x^T \mathbf H(x) \Delta x = 0\)
        \item Неопределенной, если \(\exists \Delta x, \Delta \tilde x : \Delta x^T \mathbf H(x) \Delta x > 0, \Delta \tilde x^T \mathbf H(x) \Delta \tilde x < 0\)
        \item Тождественно равной нулю, если \(\forall \Delta x \ \ \Delta x^T \mathbf H(x) \Delta x = 0\)
    \end{itemize}
\end{definition}

\subsection{Свойства выпуклых множеств и выпуклых функций}

\begin{definition}
    Пусть \(x, y \in E_n\), множество точек вида \(\{z\} \subset E_n : z = \alpha x + (1 - \alpha) y\), т.е. \(z\) это отрезок \([x, y]\).
\end{definition}

\begin{definition}
    \(U \subset E_n\) выпуклое, если вместе с точками \(x, y \in U\) оно содержит весь отрезок \(z\).
\end{definition}

\begin{definition}
    Функция \(f(x)\), заданная на выпуклом множестве \(U \subset E_n\), называется:
    \begin{itemize}
        \item \textbf{выпуклой}, если:
              \[\forall x, y \in U, \alpha \in [0, 1] \ \ f(\alpha x + (1 - \alpha) y) \leq \alpha f(x) + (1 - \alpha) f(y)\]
        \item \textbf{строго выпуклой}, если:
              \[\forall x, y \in U, \alpha \in (0, 1) \ \ f(\alpha x + (1 - \alpha) y) < \alpha f(x) + (1 - \alpha) f(y)\]
        \item \textbf{сильно выпуклой} с константой \(l > 0\), если:
              \[\forall x, y \in U, \alpha \in [0, 1] \ \ f(\alpha x + (1 - \alpha) y) \leq \alpha f(x) + (1 - \alpha) f(y) - \frac{l}{2} \alpha (1 - \alpha) ||x - y||^2\]
    \end{itemize}
\end{definition}

\begin{prop}\itemfix
    \begin{enumerate}
        \item Функция \(f(x)\) выпуклая, если её график целиком лежит не выше отрезка, соединяющего две её произвольные точки.
        \item Функция \(f(x)\) строго выпуклая, если её график целиком лежит ниже отрезка, соединяющего две её произвольные, но не совпадающие точки.\footnote{Пример будет на следующей лекции}.
        \item Если функция сильно выпуклая, то она одновременно строго выпуклая и выпуклая.
        \item Если функция строго выпуклая, то она выпуклая.
        \item Выпуклость функции можно определить по \(\mathbf H(x)\):
              \begin{itemize}
                  \item Если \(\mathbf H(x) \geq 0 \ \ \forall x \in E_n\), то \(f(x)\) выпуклая.
                  \item Если \(\mathbf H(x) > 0 \ \ \forall x \in E_n\), то \(f(x)\) строго выпуклая.
                  \item Если \(\mathbf H(x) \geq lE\footnote{единичная матрица} \ \ \forall x \in E_n\), то \(f(x)\) сильно выпуклая.
              \end{itemize}
    \end{enumerate}
\end{prop}

\begin{prop}[выпуклых функций]\itemfix
    \begin{enumerate}
        \item Если \(f(x)\) --- выпуклая функция на множестве \(U\), то всякая точка локального минимума --- глобальный минимум на \(U\).
        \item Если выпуклая функция достигает своего минимума в двух различных точках, то она достигает минимума во всех точках отрезка, соединяющего эти точки.
        \item Если \(f(x)\) строго выпуклая функция на множестве \(U\), то она может достигать своего глобального минимума на \(U\) не более чем в одной точке.
    \end{enumerate}
\end{prop}

\subsection{Необходимое и достаточное условие безусловного экстремума}

\subsubsection{Необходимое условие экстремума первого порядка}

Пусть \(x^* \in E_n\) --- точка локального минимума\footnote{или максимума} \(f(x)\) на \(E_n\) и \(f(x)\) дифференцируема в точке \(x^*\). Тогда \(\nabla f(x)\) в точке \(x^*\) равен нулю: \(\nabla f(x^*) = 0\) или \(\frac{\partial f(x^*)}{\partial x_i} = 0 \ \ \forall i \in 1 \dots n \). Точка \(x^*\) называется \textbf{стационарной}.

\subsubsection{Необходимое условие экстремума второго порядка}

Пусть \(x^* \in E_n\) --- точка локального минимума\footnote{или максимума} \(f(x)\) на \(E_n\) и \(f(x)\) дважды дифференцируема в точке \(x^*\). Тогда \(\mathbf H(x^*)\) положительно полуопределена или отрицательно полуопределена.

\subsubsection{Достаточное условие экстремума}

Пусть \(f(x)\) в \(x^* \in E_n\) дважды дифференцируема, \(\nabla f(x^*) = 0\) и \(\mathbf H(x) > 0\) (или \(\mathbf H(x) < 0\)). Тогда \(x^*\) --- точка локального минимума\footnote{или максимума} \(f(x)\) на \(E_n\).

\subsubsection{Проверка выполнений условий экстремума}

\begin{itemize}
    \item Вычисление угловых миноров \(\mathbf H(x)\)
    \item Вычисление главных миноров \(\mathbf H(x)\)
\end{itemize}

Есть два способа это сделать:
\begin{enumerate}
    \item Исследование положительной или отрицательной определенности угловых и главных миноров \(\mathbf H(x)\).
    \item Анализ собственных значений \(\mathbf H(x)\).
\end{enumerate}

\chapter{17 марта}

\subsubsection{Критерии Сильвестра проверки достаточных условий экстремума}

\begin{enumerate}
    \item Для того, чтобы \(\mathbf H(x^*) > 0\) и \(x^*\) являлась точкой локального минимума, необходимо и достаточно, чтобы \textbf{угловые} миноры были строго положительными, т.е. \(\Delta_1 > 0, \Delta_2 > 0 \dots \Delta_n > 0\).
    \item Для того, чтобы \(\mathbf H(x^*) < 0\) и \(x^*\) являлась точкой локального максимума, необходимо и достаточно, чтобы знаки \textbf{угловых} миноров чередовались, т.е. \(\Delta_1 < 0, \Delta_2 > 0 \dots ( - 1)^n\Delta_n > 0\)
\end{enumerate}

\subsubsection{Критерии Сильвестра проверки необходимых условий экстремума}

\begin{enumerate}
    \item Для того, чтобы \(\mathbf H(x^*) \geq 0\) и \(x^*\) мог быть точкой локального минимума, необходимо и достаточно, чтобы \textbf{главные} миноры были положительными, т.е. \(\Delta_1 \geq 0, \Delta_2 \geq 0, \dots \Delta^n \geq 0\)
    \item Для того, чтобы \(\mathbf H(x^*) \leq 0\) и \(x^*\) мог быть точкой локального максимума, необходимо и достаточно, чтобы знаки \textbf{главных} миноров чередовались, т.е. \(\Delta_1 \leq 0, \Delta_2 \geq 0, \dots ( - 1)^n \Delta^n \geq 0\)
\end{enumerate}

\begin{definition}
    \textbf{Собственные значения} \(\lambda_i\) матрицы \(\mathbb H(x^*)\) находятся как корни характеристического уравнения \(|H(x^*) - \lambda E| = 0\)
\end{definition}

Если \(H(x)\) --- вещественная, симметричная матрица, то \(\lambda_i\) тоже вещественные.

\subsection{Квадратичные функции}

\begin{definition}
    Функция вида
    \begin{equation}
        f(x) = \sum_{i = 1}^n \sum_{j = 1}^n a_{ij} x_i x_j + \sum_{j = 1}^n b_j x_j + c \label{квадратичная функция}
    \end{equation}
    называется \textbf{квадратичной функцией} \(n\) переменных.
\end{definition}

Положим \(a_{ij} = a_{ji}\)\footnote{На лекции было дано \(a_{ij} = a_{ij} + a_{ji}\), но это не похоже на правду, т.к. тогда \(a_{ji} = 0 \ \ \forall i, j\). Нулевая матрица действительно симметрична, но вряд ли это подразумевалось.}, тогда \(a_{ij}\) задаёт симметричную матрицу \(A\).
\begin{equation}
    f(x) = \frac{1}{2} \ev{Ax, x} + \ev{b, x} + c \label{квадратичная функция с симметричной матрицей}
\end{equation}
, где \(b = \begin{pmatrix} b_1 & \dots & b_n \end{pmatrix}^T \in E_n\) --- вектор коэффициентов, \(x = \begin{pmatrix} x_1 & \dots & x_n \end{pmatrix}^T\)

\begin{prop}[квадратичных функций]\itemfix
    \begin{enumerate}
        \item \(\nabla f(x) = Ax + b\)
              \begin{align*}
                  \frac{\partial f}{\partial x_k} & = \frac{\partial}{\partial x_k} \left( \frac{1}{2} \sum_{j = 1}^n a_{ij} x_i x_j + \sum_{j = 1}^n b_j x_j + c \right) \\
                                                  & = \frac{1}{2} \sum_{i = 1}^n (a_{ik} + a_{ki}) x_i + b_k                                                              \\
                                                  & = \sum_{i = 1}^n a_{ki} x_i + b_k
              \end{align*}

        \item \(\mathbf H(x) = A\)

              \[\frac{\partial^2 f}{\partial x_l \partial x_k} = \frac{\partial}{\partial x_l} \left( \frac{\partial f}{\partial x_k} \right) = \frac{\partial}{\partial x_l} \left( \sum_{i = 1}^n a_{ki} x_i + b_k \right) = a_{kl}\]

        \item Квадратичная функция \(f(x)\), для которой выполнено~\eqref{квадратичная функция с симметричной матрицей}, с положительно определенной матрицей \(A\) сильно выпуклая, т.к. \(\mathbf H(x) = A\) --- симметричная и положительная определенная, а следовательно \(\lambda_i > 0\) и \(\exists \) ортонормированный базис из собственных векторов этой матрицы. В этом базисе:

              \[A = \begin{pmatrix}
                      \lambda_1 & 0         & \dots  & 0         \\
                      0         & \lambda_2 & \dots  & 0         \\
                      \vdots    & \vdots    & \ddots & \vdots    \\
                      0         & 0         & \dots  & \lambda_n
                  \end{pmatrix} \quad A - lE = \begin{pmatrix}
                      \lambda_1 - l & 0             & \dots  & 0             \\
                      0             & \lambda_2 - l & \dots  & 0             \\
                      \vdots        & \vdots        & \ddots & \vdots        \\
                      0             & 0             & \dots  & \lambda_n - l
                  \end{pmatrix}\]

              В этом базисе все угловые миноры матрицы \(A\) и матрицы \(A - lE\) положительны при достаточно малом \(l : 0 < l < \lambda_{\min} \Rightarrow f\) сильно выпуклая.
    \end{enumerate}
\end{prop}

\subsection{Общие принципы многомерной оптимизации}

Алгоритмы многомерной оптимизации обычно используют итерационную процедуру, описываемую следующим образом: \(x^{k + 1} = \Phi(x^k, x^{k - 1} \dots x^0), x^0 \in E_n\). Эти алгоритмы строят последовательность промежуточных результатов \(\{x_k\}\), которая обладает следующими свойствами:
\begin{equation}
    \begin{cases}
        \lim\limits_{k \to +\infty} f(x^k) = f^* = \min\limits_{E_n} f(x), & \text{если \(U^* \neq \emptyset\)} \\
        \lim\limits_{k \to +\infty} f(x^k) = f^* = \inf\limits_{E_n} f(x), & \text{если \(U^* = \emptyset\)}
    \end{cases}
    \label{последовательность результатов}
\end{equation}
, где \(U^*\) --- множество точек глобального минимума функции \(f(x)\).

\begin{definition}
    Если для \(\{x^k\} \) выполняется условие~\eqref{последовательность результатов}, то эта последовательность называется \textbf{минимизирующей}.
\end{definition}

\begin{definition}
    Если для \(U^* \neq \emptyset\) выполняется условие \(\lim_{k \to +\infty} \rho(x^k, U^*) = 0\), то \(\{x^k\}\) \textbf{сходится} к множеству \(U^*\)
\end{definition}

\begin{definition}[расстояние от точки до множества]
    \(\rho(x, U) = \inf_{y\in U} \rho(x, y)\)
\end{definition}

Если \(U^*\) состоит из одной точки \(x^*\), то для \(\{x^k\}\), сходящейся к \(U^*\), \(\lim_{k \to +\infty} x^k = x^*\). Минимизирующая последовательность может и не сходиться к точке минимума.

\begin{theorem}[Вейерштрасса]
    Если \(f*x\) непрерывна в \(E_n\) и множество \(U^\alpha = \{x : f(x) \leq \alpha\} \) для некоторого \(\alpha\) непусто и ограничено, то \(f(x)\) достигает глобального минимума в \(E_n\).
\end{theorem}

\subsection{Скорость сходимости минимизирующей последовательности}

\begin{definition}
    \(\{x^k\} \) сходится к точке \(x^*\) \textbf{линейно} \textit{(со скоростью геометрической прогрессии)}, если
    \[\exists q \in (0, 1) : \rho(x^k, x^*) = q \rho(x^{k - 1}, x^*)\]
    , т.е. \(\rho(x^k, x^*) \leq q^k \rho(x^0, x^*)\)
\end{definition}

\begin{definition}
    Сходимость называется \textbf{сверхлинейной}, если
    \[\rho(x^k, x^*) \leq (c \rho(x^{k - 1}, x^*))^2, c > 0\]
\end{definition}

Критерий окончания итерационного процесса:
\begin{enumerate}
    \item \(\rho(x^{k + 1}, x^k) < \varepsilon_1\)
    \item \(|f(x^{k + 1}) - f(x^k)| < \varepsilon_2\)
    \item \(||\nabla f(x^k)|| < \varepsilon_3\)
\end{enumerate}

\begin{equation}
    x^{k + 1} = x^k + \alpha_k p^k
    \label{итерационный процесс 2}
\end{equation}
, где \(p^k\) --- \textbf{направление поиска} из \(x^k\) в \(x^{k + 1}\), а \(\alpha_k\) --- \textbf{величина шага}. Алгоритмы, которые мы будем рассматривать, различаются этими двумя величинами.

\begin{definition}
    В итерационном процессе~\eqref{итерационный процесс 2} производится \textbf{исчерпывающий спуск}, если величина шага \(\alpha_k\) находится из решения одномерной задачи минимизации
    \begin{equation}
        \Phi_k(\alpha) \to \min_\alpha, \Phi_k(\alpha) = f(x^k + \alpha p^k)
        \label{исчерпывающий спуск}
    \end{equation}
\end{definition}

\begin{theorem}
    Если функция \(f(x)\) дифференцируема в \(E_n\), то в итерационном процессе~\eqref{итерационный процесс 2} с выбором шага с исчерпывающим спуском для любого \(k \geq 1\) выполняется следующее условие:
    \begin{equation}
        \ev{\nabla f(x^{k + 1}), p^k} = 0
        \label{условие при процессе}
    \end{equation}
\end{theorem}
\begin{proof} % первое за курс, ура!
    Для \(\Phi_k(\alpha)\) необходимое условие минимума функции:
    \[\frac{d \Phi_k(\alpha)}{d \alpha} = \sum_{j = 1}^n \frac{\partial f(x^{k + 1})}{\partial x_j} \frac{d x_j^{k + 1}}{d \alpha} = 0\]
    Учитывая, что \(x^{k + 1}_j = x_j^k + \alpha p_j^k\), получаем, что \(\frac{d x_j^{k + 1}}{d \alpha} = p_j^k\)
\end{proof}

\begin{theorem}
    Для квадратичной функции \(f(x) = \frac{1}{2} \ev{Ax, x} + \ev{b, x} + c\) величина \(\alpha_k\) исчерпывающего спуска в итерационном процессе~\eqref{итерационный процесс 2} равна:
    \begin{equation}
        \alpha_k = - \frac{\ev{\nabla f(x^k), p^k}}{\ev{A p^k, p^k}} = - \frac{\ev{Ax^k + b, p^k}}{\ev{Ap^k, p^k}}
        \label{alpha k в исчерпывающем спуске}
    \end{equation}
\end{theorem}
\begin{proof}
    \begin{align*}
        x^{k + 1}           & = x^k + \alpha_k p^k             \\
        Ax^{k + 1} + b      & = Ax^k + b + \alpha_k A p^k      \\
        \nabla f(x^{k + 1}) & = \nabla f(x^k) + \alpha_k A p^k
    \end{align*}
    \begin{align*}
        \ev{\nabla f(x^{k + 1}), p^k}                      & = 0                                                 \\
        \ev{\nabla f(x^k) + \alpha_k A p^k, p^k}           & = 0                                                 \\
        \ev{\nabla f(x^k), p^k} + \ev{\alpha_k A p^k, p^k} & = 0                                                 \\
        \ev{\nabla f(x^k), p^k} + \alpha_k \ev{A p^k, p^k} & = 0                                                 \\
        \alpha_k                                           & = - \frac{\ev{\nabla f(x^k), p^k}}{\ev{A p^k, p^k}}
    \end{align*}
\end{proof}

\chapter{24 марта}

\begin{definition}
    Направление вектора \(p^k\) называется \textbf{направлением убывания} функции \(f(x)\) в точке \(x^k\), если при всех достаточно малых положительных \(\alpha\) выполняется неравенство \(f(x^k + \alpha p^k) < f(x^k)\)
\end{definition}

\begin{theorem}[достаточное условие направления убывания]
    Пусть функция \(f(x)\) дифференцируема в точке \(x^k\). Если вектор \(p^k\) удовлетворяет условию \(\ev{\nabla f(x^k), p^k} < 0\), то направление вектора \(p^k\) является направлением убывания.
\end{theorem}
\begin{proof}
    Из свойства дифференцируемости функции и условия теоремы следует, что
    \[f(x^{k + 1}) - f(x^k) = f(x^k + \alpha p^k) - f(x^k) = \ev{\nabla f(x^k), \alpha p^k} + \smallO(\alpha) = \alpha(\nabla f(x^k), p^k + \frac{\smallO(\alpha)}{\alpha} ) < 0\]
    при всех достаточно малых \(\alpha > 0\), т.е. \(p^k\) задает направление убывания \(f(x)\) в точке \(x^k\).
\end{proof}

Геометрическая интерпретация: \(\ev{\nabla f(x^k), p^k} < 0 \Rightarrow p^k\) составляет тупой угол с \(\nabla f(x^k)\).

Рассмотрим \(f(x)\), дифференцируемую в \(E_n\) и запишем итерационную процедуру минимизации:
\begin{equation}
    x^{k + 1} = x^k + \alpha_k p^k
    \label{итерационный процесс}
\end{equation}
, где \(p^k\) определяется с учетом информации о частных производных, а величина \(\alpha_k\) такова, что:
\begin{equation}
    f(x^{k + 1}) < f(x^k)
    \label{уменьшение f}
\end{equation}

Условие остановки итерационного процесса: \(||\nabla f(x^k)|| < \varepsilon\).

\subsection{Метод градиентного спуска}

Предпололжим, что в~\eqref{итерационный процесс} \(p^k = - \nabla f(x^k)\). Если \(\nabla f(x^k) \neq 0\), то \(\ev{\nabla f(x^k), p^k} < 0\), следовательно \(p^k\) --- направление убывания \(f(x)\), причём в малой окрестности точки \(x^k\) направление \(p^k\) обеспечивает наискорейшее \textbf{убывание} функции. Таким образом, \(\exists \alpha_k > 0\), такое что \eqref{уменьшение f} выполнено.

Алгоритм метода:
\begin{enumerate}
    \item Выбрать \(\varepsilon > 0, \alpha > 0, x\in E_n\), вычислить \(f(x)\).
    \item Вычислить \(\nabla f(x)\). Проверить условие \(||\nabla f(x)|| < \varepsilon\). Если оно выполнено, то завершить процесс, иначе перейти к шагу 3.
    \item Найти \(y = x - \alpha \nabla f(x)\) и \(f(y)\). Если \(f(y) < f(x)\), то положить \(x = y, f(x) = f(y)\) и перейти к шагу 2, иначе --- к 4.
    \item Положить \(\alpha = \frac{\alpha}{2}\) и перейти к шагу 3.
\end{enumerate}

\begin{remark}
    В окрестности стационарной точки величина градиента мала, вследствие чего сходимость процесса замедляется. Поэтому в~\eqref{итерационный процесс} иногда полагают
    \[p^k = - \frac{\nabla f(x^k)}{||\nabla f(x^k)||}\]
\end{remark}

\begin{theorem}
    Пусть симметричная матрица \(A\) квадратичной функции \(f(x)\) положительно определена, \(l\) и \(L\) --- наименьшее и наибольшее собственные значения \(A\) (\(0 < l \leq L\)). Тогда при любых \(\alpha \in \left( 0, \frac{2}{L} \right)\) и \(x^0 \in E_n\) \eqref{итерационный процесс} сходится к единственной точке глобального минимума \(x^*\) функции \(f(x)\) линейно:
    \[\rho(x^k, x^*) \leq q^k \rho(x^0, x^*)\]
\end{theorem}
\begin{proof}
    Т.к. \(A\) положительно определена, то \(f(x)\) сильно выпукла. Следовательно точка \(x^*\) существует и единственна. \(\nabla f(x^*) = 0\), тогда:
    \[\nabla f(x^k) = Ax^k + b = Ax^k + b - Ax^* - b = A(x^k - x^*)\]
    \begin{align*}
        ||x^k - x^*|| & = ||x^{k - 1} - \alpha \nabla f(x^{k - 1}) - x^*|| \\
                      & = ||x^{k - 1} - x^* - \alpha A(x^{k - 1} - x^*)||  \\
                      & = ||(E - \alpha A)(x^{k - 1} - x^*)||
    \end{align*}
    \begin{align*}
        ||x^k - x^*|| & \leq ||E - \alpha A|| \cdot ||x^{k - 1} - x^*|| \\
                      & \leq q ||x^{k - 1} - x^*||                      \\
                      & \leq q^k ||x^0 - x^*||
    \end{align*}
\end{proof}

\(q\) --- оценка нормы матрицы через величину её собственных значений: \(||E - \alpha A|| \leq q = \max \{|1 - \alpha l|, |1 - \alpha L|\} \).  Величина \(q\) принимает наименьшее значение при \(q^* = \frac{L - l}{L + l}\) при \(\alpha = \alpha^* = \frac{2}{L + l}\)
\begin{proof}
    Т.к. \(l < L\), то \(1 - \alpha l = -(1 - \alpha L)\). Тогда \(q = 1 - \alpha l = 1 - \frac{2l}{L + l} = \frac{L - l}{L + l}\)
\end{proof}

От соотношения \(l\) и \(L\) существенно зависит число итераций градиентного метода при минимизации выпуклой квадратичной функции.

\begin{example}[\(L = l > 0\)]
    \(f(x) = x_1^2 + x_2^2 \to \min, x^0 = \begin{pmatrix} 1 & 1 \end{pmatrix}^T, \alpha = \alpha^*\)

    Решение:
    \[A = \begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix} \Rightarrow l = L = 2 \Rightarrow \alpha^* = \frac{2}{2 + 2} = \frac{1}{2}\]
    \[x^1 = x^0 - \frac{1}{2} \nabla f(x^0) = \begin{pmatrix} 0 & 0 \end{pmatrix}^T\]

    Несложно заметить, что \(x^1 = x^*\).

    Таким образом, точка минимума нашлась за один шаг.
\end{example}

При \(l = L\) линии уровня \(f(x)\) --- концентрические окружности. При \(L \gg l > 0\) линии уровня \(f(x)\) --- эллипсы:

\begin{example}[\(L \gg l > 0\)]
    \(f(x) = x_1^2 + 100x_2^2 \to \min , x_0 = \begin{pmatrix} 1 & 1 \end{pmatrix}^T, \alpha = \alpha^*\)

    \[A = \begin{pmatrix} 2 & 0 \\ 0 & 200 \end{pmatrix} \Rightarrow l = 2, L = 200\]
    \[ - \nabla f(x^0) = \begin{pmatrix} - 2 & - 200 \end{pmatrix}^T\]
    \(x^* - x^0 = \begin{pmatrix} - 1 & - 1 \end{pmatrix}^T\) --- направление к точке глобального минимума, сильно отличается от направления спуска, минимизирующая последовательность сходится зигзагообразно.
\end{example}

\begin{definition}
    \textbf{Число обусловленности} для симметричной положительно определенной матрицы: \(\mu = \frac{L}{l}\). Оно характеризует степень вытянутости линий уровня \(f(x) = C\).
\end{definition}

\begin{itemize}
    \item Если \(\mu\) велико, то линии уровня сильно вытянуты, функция имеет овражный характер, т.е. резко меняется по одним направлениям и слабо по другим. В таком случае задачу минимизации называют плохо обусловленной.
    \item Если \(\mu \sim 1\), линии уровня близки к окружности и задача называется хорошо обусловленной.
\end{itemize}

\subsection{Метод наискорейшего спуска}

Идея: после вычисления в начальной точке градиента функции делает в направлении антиградиента не малый шаг, а передвигается до тех пор, пока функция убывает. Достигнув точки минимума на выбранном направлении, повторяет описанную процедуру.

\(\alpha_k\) находится из решения задачи одномерной оптимизации:
\begin{equation}
    \Phi_k(\alpha) \to \min , \Phi_k(\alpha) = f(x^k - \alpha \nabla f(x^k)), \alpha > 0
    \label{задача оптимизации}
\end{equation}

Алгоритм метода:
\begin{enumerate}
    \item Выбрать \(\varepsilon > 0, x^0 \in E_n\), вычислить \(f(x^0)\)
    \item Вычислить \(\nabla f(x)\). Проверить условие \(||\nabla f(x)|| < \varepsilon\). Если оно выполнено, то завершить процесс, иначе перейти к шагу 3.
    \item Решить задачу~\eqref{задача оптимизации} для \(x^k = x\), т.е. найти \(\alpha^*\). Положить \(x = x - \alpha^* \nabla f(x)\), перейти к шагу 2.
\end{enumerate}

\begin{definition}
    Ненулевые вектора \(p^1 \dots p^k\) называются \textbf{сопряженными} относительно матрицы \(A\) размера \(n \times n\) или \textbf{\(A\)-ортогональными}, если \(\ev{A p^i, p^j} = 0\), если \(i \neq j\).
\end{definition}

Система из \(n\) векторов \(p^1 \dots p^n\), сопряженных относительно положительно определенной матрицы \(A\), линейно независима и образует базис в \(E_n\).

Рассмотрим минимизацию квадратичной функции \(f(x) = \frac{1}{2} \ev{Ax, x} + \ev{b, x} + c\) в \(E_n\), где \(A\) положительно определенная и итерационный процесс \eqref{итерационный процесс}, где \(p^k\) --- \(A\)-ортогональные.

Если в таком итерационном процессе на каждом шаге исчерпывающий спуск, то:
\[\alpha_k = -\frac{\ev{\nabla f(x^0), p^k}}{\ev{A p^k, p^k}} \]

\begin{proof}
    \[x^k = x^{k - 1} + \alpha_k p^k = x^0 + \sum_{i = 1}^n \alpha_i p^i\]
    \[\nabla f(x) = Ax + b\]
    \[\nabla f(x^k) = \nabla f(x^0) + \sum_{i = 1}^k \alpha_i A p^i\]
    Домножим на \(p^k\):
    \[\ev{\nabla f(x^k)} = \ev{\nabla f(x^0), p^k} + \ev{\alpha_k A p^k, p^k} \]
    \[\ev{\nabla f(x^0), p^k} + \ev{\alpha_k A p^k, p^k} = 0\]
    Т.к. \(A\) положительно определено, \(\ev{A p^k, p^k} > 0\) и для \(\alpha_k\):
    \[\alpha_k = -\frac{\ev{\nabla f(x^0), p^k}}{\ev{A p^k, p^k}} \]
\end{proof}

\begin{theorem}
    Последовательный исчерпывающий спуск по \(A\)-ортогональным направлениям приводит квадратичной формы не более чем за \(n\) шагов.
\end{theorem}

\subsection{Метод сопряженных градиентов}

\begin{equation}
    p^{k + 1} = - \nabla f (x^{k + 1}) + \beta_k p^k
    \label{процесс сопряженных}
\end{equation}
\(\beta_k\) выбираются так, чтобы получалась последовательность \(A\)-ортогональных векторов \(p^0, p^1 \dots \). Из условия \(\ev{A p^{k + 1}, p^k} = 0\) имеем:
\begin{equation}
    \beta_k = \frac{\ev{A \nabla f(x^{k + 1}, p^k)}}{\ev{A p^k, p^k}}
    \label{beta}
\end{equation}

Для квадратичной функции:
\begin{equation}
    \alpha_k = -\frac{\ev{\nabla f(x^k), p^k}}{\ev{A p^k, p^k}}
    \label{alpha}
\end{equation}

Вышеуказанный итерационный процесс дает точки \(x^0 \dots x^k\) и векторы \(p^0 \dots p^k\), такие, что если \(\nabla f(x^i) \neq 0\) при \(0 \leq i < k \leq n - 1\), то векторы \(p^0 \dots p^k\) \(A\)-ортогональны, а \(\nabla f(x^0) \dots \nabla f(x^i)\) взаимно ортогональны.

Т.к. в~\eqref{процесс сопряженных} \(p^k\) \(A\)-ортогональны, то метод гарантирует нахождение точки минимума сильно выпуклой функции \underline{не более, чем за \(n\) шагов}.

Следующие формулы описывают итерационный процесс метода сопряженных градиентов:
\begin{equation}
    x^{k + 1} = x^k + \alpha_k p^k \quad x^0 \in E_n, p^0 = -\nabla f(x^0)
\end{equation}
\begin{equation}
    f(x^k + \alpha_k p^k) = \min_{\alpha > 0} f(x^k + \alpha p^k)
\end{equation}
\begin{equation}
    p^{k + 1} = - \nabla f(x^{k + 1} + \beta_k p^k)
\end{equation}
\begin{equation}
    \beta_k = \frac{||\nabla f(x^{k + 1})||^2}{||\nabla f(x^k)||^2}
\end{equation}
Можем заметить, что мы не используем матрицу \(A\), поэтому этот метод может применяться для минимизации не только квадратичных функций. Но этот метод может не находить точку минимума не квадратичной функции за конечное число шагов.

Вектора \(p^k\) вообще говоря могут не образовывать \(A\)-ортогональную систему, вследствие чего реализация этого метода будет сопровождаться неизбежными накапливающимся погрешности, из-за чего сходимость метода может нарушиться. Чтобы с этим бороться, через каждые \(N\) шагов производят обновление метода, т.е. \(\beta_{m \cdot N} = 0, m\in\N\), где \(m \cdot N\) называются моментами обновления метода \textit{(рестарта)}, а \(N\) обычно принимают за \(n\) --- размерность пространства \(E_n\).

\subsection{Метод стохастического градиентного спуска}

Этот метод используется, когда дано множество пар \((x,y)\), называемых тренировочными наборами. Это множество разделяется на \(K\) подмножеств размера \(M\), называемых minibatch.\footnote{Проще говоря, этот метод используется в машинном обученнии}

\[X^{(k)} = \{x_i\ |\ i = M_k, \dots (M_k + M - 1)\}\]
\[Y^{(k)} = \{y_i\ |\ i = M_k, \dots (M_k + M - 1)\}\]
\[L^{(k)}(w) = \sum_{i = 0}^M L(w, x_{M_k + i}, y_{M_k + i})\]


Есть большие итерации по \(p\), называемые \textbf{эпохами} и малые итерации \(w_p^{(k + 1)} = w_p^{(k)} - \eta \cdot \nabla L^{(k)}(w_p^{(k)}), w_{p + 1}^{(0)} = w_p^{(K)}\), где \(\eta = \const\)\footnote{И называется learning rate}. При переходе от одной эпохе к другой minibatch-и случайно перемешиваются.

\subsubsection{Adagrad}

\begin{remark}
    Куда более адекватная статья по adagrad и прочим модификациям стохастического спуска: \href{https://habr.com/ru/post/318970/}{https://habr.com/ru/post/318970/}
\end{remark}

\begin{remark}
    Лучшая модификация SGD --- Adam, если не хочется думать, надо всегда использовать его.
\end{remark}

Идея алгоритма --- в покоординатном изменении \(\eta\). Пусть \(\eta_p = \begin{pmatrix} \eta_p^{(1)} & \dots & \eta_p^{(d)} \end{pmatrix} \), \(\eta_0\) --- константный вектор \(\eta_0^{(i)} = \eta \ \ \forall i\).

Вспомогательные данные:
\[\nabla L(w_p) = \begin{pmatrix} g_p^{(1)} & \dots & g_p^{(d)} \end{pmatrix} \quad G_p^{(i)} = \sum_{j = 1}^p (g_j^{(i)})^2\]

Тогда пусть
\[\eta_p^{(i)} = \frac{\eta}{\sqrt{G^{(i)}_p + \epsilon}}\]
, где \(\epsilon \approx 10^{ - 8}\) --- сглаживающий параметр, который позволяет избежать деления на \(0\).

Тогда правило перехода будет:
\[w_{p + 1} = w_p - \eta_p \odot \nabla L(w_p)\]
, где \(\odot\) --- поэлементное умножение векторов.

\subsection{Метод покоординатного спуска}

Алгоритм метода:
\begin{enumerate}
    \item Фиксируем значения всех переменных вектора \(x = \begin{pmatrix} x_1 & \dots & x_n \end{pmatrix} \), кроме \(x_i\).
    \item \(f(x_i) \to \min\) методом одномерной оптимизации \textit{(наиболее популярный метод --- золотого сечения)}.
    \item Проверка критерия остановки:
        \begin{itemize}
            \item \(||x^{k + 1} - x^k|| \leq \varepsilon_1\)
            \item \(||f(x^{k + 1}) - f(x^k)|| \leq \varepsilon_2\)
        \end{itemize}
\end{enumerate}

\end{document}
