\chapter{22 ноября}

\subsection{Построение и анализ уравнения множественной линейной регрессии}

Пусть выявлена зависимость признака--результата \(X\) от факторов \(Z_1 \dots Z_k\). При \(n \geq k\) экспериментов получены экспериментальные результаты \(\vec{X} = (X_1 \dots X_n)\) при значениях факторов \(\vec{Z}^{(i)} = (Z_1^{(i)} \dots Z_k^{(i)})\) --- значения при \(i\)--том эксперименте, \(1 \leq i \leq n\). Предполагаем, что зависимость \(X\) ото всех факторов --- линейная. Цель: по этим данным построить модель, наилучшим образом объясняющую и предсказывающую поведение \(X\).

\subsubsection{Мультиколлинеарность}

%<*44>
\begin{definition}
    \textbf{Мультиколлинеарность} --- наличие заметной линейной связи между всеми или несколькими факторами.
\end{definition}

% Когда в построенной модели есть мультиколлинеарность, то стандартные ошибки параметров будут ненадежными (будут иметь большую дисперсию) и на основании такой модели трудно вычленить влияние отдельного фактора и из-за этого нельзя делать какие-то выводы о причинах связи.

Неприятные последствия:
\begin{enumerate}
    \item Оценки параметров становятся ненадежными --- имеют большие стандартные ошибки и малую значимость\footnote{Об этом --- в конце лекции.}.
    \item Небольшое изменение исходных данных существенно влияет на изменение оценок регрессии.
    \item Трудно выявить изолированное влияние конкретного фактора на результат и физический (экономический) смысл этого влияния.
\end{enumerate}

\subsubsection{Начальный отбор факторов модели}

Находим корреляционную матрицу, состоящую из коэффициентов линейной корреляции:
\[R = \begin{pmatrix}
        1          & r_{X,Z_1}    & r_{X,Z_2}   & \dots  & r_{X,Z_k}   \\
        r_{X,Z_1}  & 1            & r_{Z_1,Z_2} & \dots  & r_{Z_1,Z_k} \\
        \vdots     & \vdots       & \ddots      &        & \vdots      \\
        \vdots     & \vdots       &             & \ddots & \vdots      \\
        r_{X, Z_k} & r_{Z_1, Z_k} & \dots       & \dots  & 1           \\
    \end{pmatrix}\]

Алгоритм:
\begin{enumerate}
    \item Берём фактор, наиболее коррелирующий с \(X\).
    \item По очереди добавляем факторы со свойствами:
          \begin{enumerate}
              \item Корреляция с \(X\) как можно большая.
              \item Корреляция с раннее введенными факторами как можно меньше.
          \end{enumerate}
\end{enumerate}

\begin{example}\itemfix
    \begin{center}
        \begin{tabular}{CCCCC}\toprule
                & X     & Z_1   & Z_2   & Z_3 \\ \midrule
            X   & 1     & -     & -     & -   \\
            Z_1 & 0.85  & 1     & -     & -   \\
            Z_2 & 0.81  & 0.93  & 1     & -   \\
            Z_3 & -0.65 & -0.43 & -0.19 & 1   \\
            \bottomrule
        \end{tabular}
    \end{center}

    В модель вводится \(Z_1, Z_3\)
\end{example}
%</44>

\subsubsection{Анализ уравнения линейной регрессии}

%<*45.1>
Пусть:
\begin{itemize}
    \item \(X = \beta_0 + \beta_1 Z_1 + \dots \beta_k Z_k + \varepsilon\) --- теоретическая модель.
    \item \(\hat{X} = b_0 + b_1 Z_1 + \dots + b_k Z_k\) --- уравнение, полученное методом наименьших квадратов.
\end{itemize}

Как и раньше, считаем, что \(\varepsilon \in N(0, \sigma^2)\).

Согласно пункту~\ref{ОТМЛР3} теоремы~\ref{th:основная теорема множественной линейной регрессии} \(S^2 = \frac{1}{n - k - 1\footnotemark} \sum_{i=1}^{k} \hat{\varepsilon}_i^2\)\footnotetext{Из-за свободного члена.} --- несмещённая оценка для \(\sigma^2\), где \(\varepsilon_i = X_i - \hat{X}_i\).

\begin{definition}
    \(S\) --- \textbf{стандартная ошибка регрессии}.
\end{definition}

Из свойства~\ref{prop:ОМНК5} имеем \(\D b_i = \sigma^2 (A^{-1})_{ii}\), где \(A = ZZ\tran, Z\) --- матрица плана. Соответственно \(\hat{\D}\,b_i = S^2 (A^{-1})_{ii}\) --- оценка \(\D b_i\).

\(S_{b_i} = S\sqrt{(A^{-1})_{ii}}\) --- \textbf{стандартная ошибка коэффициента \(b_i\)}.
%</45.1>

\subsubsection{Уравнение регрессии в стандартных масштабах}

%<*46>
Так как факторы имеют различную природу и измеряются в различных единицах, то по коэффициентам уравнения МНК нельзя судить о силе влияния каждого фактора. Поэтому удобно стандартизовать исследуемые величины:
\[t_X \coloneqq \frac{X - \overline{X}}{\sigma_X} \quad t_j \coloneqq \frac{Z_{j}^{(i)} - \overline{Z_j}}{\sigma_{Z_j}}, 1 \leq j \leq k\]

Если в уравнении регрессии заменить величины стандартизованными величинами, то получим уравнение стандартных масштабов:
\[t_X = \gamma_1 t_1 + \dots \gamma_k t_k\]
\begin{remark}
    \(\gamma_0 = 0\), т.к. \(\underbrace{\overline{t_k}}_{= 0} = \gamma_1 \underbrace{\overline{t_1}}_{= 0} + \dots + \gamma_k \underbrace{\overline{t_k}}_{= 0} + \gamma_0\)
\end{remark}

При этом система нормальных уравнений приобретает простой вид:
\[\begin{cases}
        \gamma_1 + r_{Z_1, Z_2} \gamma_2 + r_{Z_1, Z_3} \gamma_3 + \dots + r_{Z_1, Z_k} \gamma_k = r_{Z_1, X} \\
        r_{Z_2, Z_1} \gamma_1 + \gamma_2 + r_{Z_2, Z_3} \gamma_3 + \dots + r_{Z_2, Z_k} \gamma_k = r_{Z_2, X} \\
        \vdots                                                                                                \\
        r_{Z_k, Z_1} \gamma_1 + r_{Z_k, Z_2} \gamma_2 + \dots + \gamma_k = r_{Z_k, X}
    \end{cases}\]
Или в матричной форме:
\[R \Gamma = R_X\]
, где \(R\) --- матрица корреляции, \(\Gamma = \begin{pmatrix}
    \gamma_1 \\
    \vdots   \\
    \gamma_k
\end{pmatrix}, R_X = \begin{pmatrix}
    r_{X, Z_1} \\
    \vdots     \\
    r_{X, Z_k}
\end{pmatrix}\)

\(\gamma_i\) и \(b_i\) связаны соотношением:
\begin{myemph}
    b_i = \gamma_i \cdot \frac{\sigma_X}{\sigma_{Z_i}}
\end{myemph}

Частный случай --- уравнение парной линейной регрессии:
\[\frac{X - \overline{X}}{\sigma_X} = r_{\text{в}} \cdot \frac{Z - \overline{Z}}{\sigma_Z}\]
В данном случае \(\gamma_1 = r_{\text{в}}\)

Смысл стандартизованных коэффициентов \(\gamma_i\): \(\gamma_i\) показывает, на какую часть своего среднего отклонения \(\sigma_X\) изменится результат \(X\) при изменении фактора \(Z_i\) на величину своего среднего отклонения \(\sigma_{Z_i}\).

При мультиколлинеарности факторы результата оказывают не только прямое воздействие на результат, но и косвенное через влияние других факторов. Стандартизованный коэффициент \(\gamma_i\) можно трактовать как показатель прямого влияния фактора \(Z_i\) на результат. Косвенное влияние этого фактора показывают остальные слагаемые: \(\sum_{j \neq i} \gamma_j r_{Z_i, Z_j}\).
\[\overbrace{r_{Z_i, Z_1} \gamma_1 + r_{Z_i, Z_2} \gamma_2 + \dots}^{\text{косвенное влияние}} + \underbrace{\gamma_i}_{\mathclap{\text{прямое влияние}}} + \overbrace{r_{Z_i, Z_{i+1}} \gamma_{i+1} + \dots + r_{Z_i, Z_k} \gamma_k}^{\text{косвенное влияние}} = r_{Z_i, X}\]

\begin{remark}
    Для измерения тесноты линейной связи между конкретным фактором и результатом при устранении других факторов есть понятие коэффициента частной корреляции.
\end{remark}

\subsection{Частные коэффициенты эластичности}

\begin{definition}
    Пусть имеется уравнение регрессии \(X = f(Z_1 \dots Z_k)\). \textbf{Частными коэффициентами эластичности} называются величины
    \begin{myemph}
        \textit{Э}_i = \frac{\partial f}{\partial Z_i} \cdot \frac{\overline{Z}_i}{\overline{X}}
    \end{myemph}
\end{definition}
Смысл: \(\textit{Э}_i\) показывает, на сколько процентов от среднего изменится \(X\) при изменении \(Z_i\) на 1\% от своего среднего уровня при фиксированных значениях других факторов. В случае линейной регрессии:
\begin{myemph}
    \textit{Э}_i = b_i \cdot \frac{\overline{Z}_i}{\overline{X}}
\end{myemph}

\begin{remark}
    Коэффициенты эластичности и стандартизованные коэффициенты могут привести к противоположным выводам. Причины:
    \begin{enumerate}
        \item Вариация фактора очень велика.
        \item Воздействие факторов разнонаправленно.
        \item Мультиколлинеарность.
    \end{enumerate}
\end{remark}
%</46>

\subsection{Коэффициенты детерминации и множественной корреляции}

%<*47>
Допустим, что дисперсию результата \(X\) можно разложить на объясненную моделью дисперсию и дисперсию остатков:
\[\D X = \D \hat{X} + \D \varepsilon\]

\begin{definition}
    \textbf{Коэффициентом детерминации} \(R^2\) называется:
    \begin{myemph}
        R^2 = \frac{\D \hat{X}}{\D X}
    \end{myemph}
    \begin{center}
        или
    \end{center}
    \begin{myemph}
        R^2 = 1 - \frac{\D \varepsilon}{\D X}
    \end{myemph}
\end{definition}
\begin{prop}\itemfix
    \begin{enumerate}
        \item \(0 \leq R^2 \leq 1\)
        \item Если \(R^2 = 1\), то \(\D \varepsilon = 0\) и т.к. \(\overline{\varepsilon} = 0\), то \(\varepsilon_i = 0\) для всех \(i\), т.е. все наблюдаемые точки лежат в гиперплоскости регрессии.
        \item Если \(R^2 = 0\), то \(\D \hat{X} = 0\), \(\forall i \ \ X_i = \overline{X}\) и \(b_1 = \dots = b_k = 0\).\label{prop:R2 3}
        \item Чем больше \(R^2\), тем лучше модель.
        \item В случае линейного уравнения регрессии:
              \[R^2 = \sum_{i=1}^{k} \gamma_i r_{X, Z_i}\]
    \end{enumerate}
\end{prop}

\begin{definition}
    \(R = \sqrt{R^2}\) --- \textbf{коэффициент множественной корреляции}.
\end{definition}

\begin{remark}
    При добавлении в модель нового фактора \(R^2\) почти всегда увеличивается. Поэтому для выяснения необходимости введения новых факторов в модель используется \textbf{скорректированный коэффициент детерминации}:
    \[\overline{R^2} = 1 - \frac{n - 1}{n - k - 1}\frac{\D \varepsilon}{\D X}\]
    , где \(n\) --- число экспериментов, \(k\) --- число факторов в модели.
\end{remark}

\subsection{Проверка гипотезы о значимости уравнения регрессии в целом}

Проверяется гипотеза \(H_0 : R^2_T = 0\) против гипотезы \(H_1 : R^2_T \neq 0\), т.е. уравнение статистически значимо.

\begin{theorem}
    Если \(H_0\) верна, то:
    \[F = \frac{R^2}{1 - R^2}\frac{n - k - 1}{k} \in F(k, n - k - 1)\]
\end{theorem}

\underline{Критерий}: \(t_{\text{кр}}\) --- квантиль \(F(k, n - k - 1)\) уровня значимости \(\alpha\). Тогда:
\[\begin{cases}
        H_0, & F < t_{\text{кр}}    \\
        H_1, & F \geq t_{\text{кр}}
    \end{cases}\]
\begin{remark}
    По свойству~\ref{prop:R2 3} это эквивалентно проверке гипотезы \(H_0 : \beta_1 = \beta_2 = \dots = \beta_k = 0\)
\end{remark}
%</47>

\subsection{Проверка гипотезы о значимости отдельного коэффициента регрессии}

%<*45.2>
Проверяется основная гипотеза \(H_0 : \beta_i = 0\) против \(H_1 : \beta_i \neq 0\).

\begin{theorem}
    Если \(H_0\) верна, то \(T_i = \frac{b_i}{S_{b_i}} \in T_{n - k - 1}\), где \(T_{n - k - 1}\) --- распределение Стьюдента с \(n - k - 1\) степенями свободы, а \(S_{b_i}\) --- стандартная ошибка коэффициента \(b_i\).
\end{theorem}
\underline{Критерий}: \(t_{\text{кр}}\) --- квантиль \(|T_{n - k - 1}|\) уровня значимости \(\alpha\). Тогда:
\[\begin{cases}
        H_0 : T_i < t_{\text{кр}} \Rightarrow Z_i \text{ можно выкинуть из модели} \\
        H_1 : T_i \geq t_{\text{кр}} \Rightarrow Z_i \text{ можно оставить в модели}
    \end{cases}\]

При большой мультиколлинеарности может случиться так, что все коэффициенты статистически не значимы, а уравнение в целом значимо.
%</45.2>
