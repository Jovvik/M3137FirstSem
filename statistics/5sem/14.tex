\chapter{6 декабря}

\section{Моделирование случайных величин}

\subsection{Датчики случайных чисел}

\begin{definition}
    Пусть случайная величина \(\eta \in U(0, 1)\). Пусть члены последовательности \(y_1 \dots y_n\) можно рассматривать, как экспериментальные значения данной величин. Тогда члены этой последовательности называются \textbf{псевдослучайными числами}, а устройство (или алгоритм для их получения) называется \textbf{датчиком случайных чисел}.
\end{definition}

\subsubsection{Физические датчики}

\begin{itemize}
    \item Рулетка (из казино), часы в миллисекундах
    \item Монета
\end{itemize}

\begin{theorem}
    Случайная величина \(\eta \in U(0, 1)\) тогда и только тогда, когда разряды её двоичной записи \(\sum_{i=1}^{\infty} 2^{ - i} \xi_i\) --- схема Бернулли с \(p = \frac{1}{2}\).
\end{theorem}
\begin{corollary}
    Если требуется точность \(2^{-n}\), то \(n\) раз бросаем монету и в двоичной записи пишем в \(i\)-тый разряд ставим единицу, если орёл, и ноль иначе.
\end{corollary}

\subsubsection{Таблица случайных чисел}

Пусть имеется таблица чисел (обычно двухзначных) --- результат работы некоторого датчика случайных чисел. Наугад выбираем номер строки и столбца и начиная с этого числа выписываем последовательность чисел нужной длины. Если требуется большая точность, то приписываем соответствующие цифры из соседнего столбца.

\subsubsection{Математические датчики}

%<*50>
Наиболее популярны мультипликативные датчики.

Задается начальное число \(k_0\), множитель \(a\) и \(m\) --- делитель (модуль). При этом \(\gcd(k_0, m) = \gcd(a, m) = 1, 0 < a, k_0 < m\). Последовательность случайных чисел задается следующим образом:
\[k_n = ak_{n-1} \pmod m\]
Псевдослучайным числом тогда будет \(\frac{k_n}{m}\).

\begin{remark}
    Эта процедура зациклится не более, чем за \(m - 1\) итераций. Точнее, не более, чем за \(\varphi\footnote{Функция Эйлера.}(m)\).
\end{remark}

\subsubsection{Рекомендации}

Для 32-х битных компьютеров:
\begin{itemize}
    \item \(m = 2^{31} - 1 = 2\ 147\ 483\ 647\)
    \item \(a = 630\ 360\ 016\) или \(a = 764\ 261\ 123\)
    \item \(k_0\) --- не важно.
\end{itemize}

\subsubsection{Датчик Уичмана и Хилла}

Одновременно запускаются три мультипликативных датчика с параметрами:

\begin{center}
    \begin{tabular}{LL}\toprule
        a   & m       \\ \midrule
        171 & 30\ 269 \\
        172 & 30\ 307 \\
        170 & 30\ 323 \\
        \bottomrule
    \end{tabular}
\end{center}

На \(n\)-том шаге получаем три числа: \(y_n', y_n'', y_n'''\). Тогда \(y_n \coloneqq \left\{y_n' + y_n'' + y_n'''\right\}\) --- дробная часть их суммы.

Преимущества:
\begin{enumerate}
    \item Период \(3 \cdot 10^{13}\), а у предыдущего \(1 \cdot 10^9\).
    \item Быстрее вычисляется.
\end{enumerate}
%</50>

\subsection{Моделирование непрерывных распределений}

\subsubsection{Метод обратных функций (квантильное преобразование)}

%<*51.1>
\begin{theorem}
    Пусть \(F(x)\) --- непрерывная строго возрастающая функция распределения. Если случайная величина \(\theta\) имеет равномерное стандартное распределение, то случайная величина \(\xi = F^{-1}(\theta)\) имеет функцию распределения \(F(x)\).
\end{theorem}

\begin{example}
    Показательное распределение \(E_\alpha\).
    \[F(x) = \begin{cases}
            0,               & x < 0    \\
            1 - e^{-x\alpha} & x \geq 0
        \end{cases}\]
    \[y = 1 - e^{ - x\alpha} \Leftrightarrow x = - \frac{\ln(1 - y)}{\alpha} \in E_\alpha \Rightarrow x_i = - \frac{1}{\alpha} \ln \eta_i\]
    Тогда \(x_i\) --- значения \(E_\alpha\) при \(\eta_i \in U(0, 1)\).
\end{example}
\begin{example}
    Нормальное распределение \(N(a, \sigma^2)\). Достаточно смоделировать стандартное нормальное распределение \(N(0, 1)\).

    \[F_0(x) = \frac{1}{\sqrt{2\pi}} \int_{ - \infty }^\alpha e^{-\frac{z^2}{2}} dz\]
    \[x_i = F_0^{-1}(\eta_i), \eta_i \in U(0, 1)\]
\end{example}

\begin{itemize}
    \item Достоинства: простота и универсальность
    \item Недостаток: неэффективность
\end{itemize}
%</51.1>

%<*52>
\subsubsection{Нормальные случайные числа на основе ЦПТ}

Пусть \(\eta_i \in U(0, 1), \E \eta_i = \frac{1}{2}, \D \eta_i = \frac{1}{12}, S_n \coloneqq \eta_1 + \dots + \eta_n\). Согласно ЦПТ\footnote{Центральной предельной теореме}:
\[\frac{S_n - na}{\sqrt{n \D \xi}} = \frac{S_n - \frac{n}{2}}{\sqrt{\frac{n}{12}}} \xrightrightarrows{n \to +\infty} Z \in N(0, 1)\]
Уже при \(n = 12\) распределение \(S_n - 6\) неплохо приближается к \(N(0, 1)\).

\subsubsection{Точное моделирование пары независимых случайных величин \(N(0, 1)\)}

\begin{theorem}
    Пусть независимые случайные величины \(\eta_1, \eta_2 \in U(0, 1)\). Тогда следующие случайные величины \(X\) и \(Y\) независимы и \(\in N(0, 1)\):
    \[X \coloneqq \sqrt{ - 2 \ln \eta_1} \cos(2\pi \eta_2) \quad Y \coloneqq \sqrt{ - 2 \ln \eta_1} \sin(2\pi \eta_2)\]
\end{theorem}
\begin{proof}
    Рассмотрим независимые случайные величины \(X, Y \in N(0, 1)\). Тогда плотность их совместного распределения это
    \[f_{X, Y} = f_X(X) = f_Y(Y) = \frac{1}{\sqrt{2 \pi}} e^{ -\frac{x^2}{2}} \frac{1}{\sqrt{2 \pi}} e^{ -\frac{y^2}{2}} = \frac{1}{2\pi} e^{ - \frac{x^2 + y^2}{2}}\]
    Перейдём к полярным координатам, где радиус \(R\), а угол --- \(\phi\), тогда \(X = R\cos \phi, Y = R\sin \phi, J = r\). Плотность в этих координатах:
    \[f_{X, Y} = \underbrace{\frac{1}{2\pi}}_{\substack{f_\phi(\varphi) \\ 0 \leq \varphi \leq 2\pi}} \underbrace{e^{ - \frac{r^2}{2}} r}_{\substack{f_R(r) \\ r \geq 0}}\]
    Таким образом, \(R\) и \(\phi\) --- независимые случайные величины. Их функции распределения:
    \[F_R(r) = \int_{-\infty}^r \rho e^{ - \frac{\rho^2}{2}} d\rho = 1 - e^{ - \frac{r^2}{2}}\]
    \[F_\phi(\varphi) = \frac{\varphi}{2\pi}\]
    \[F_R^{-1}(r) = \sqrt{ - 2\ln (1 - y)} \quad F_\phi^{-1}(\varphi) = 2\pi y\]

    Тогда \(r_i \coloneqq \sqrt{ - 2\ln \eta_1}, \varphi = 2\pi \eta_2\). Итого
    \[X = \sqrt{ - 2 \ln \eta_1} \cos(2\pi \eta_2) \quad Y = \sqrt{ - 2 \ln \eta_1} \sin(2\pi \eta_2)\]
\end{proof}
%</52>

\subsubsection{Быстрый показательный датчик}

%<*53>
\begin{theorem}
    Пусть независимые случайные величины \(\eta_1 \dots \eta_{2n - 1} \in U(0, 1)\). Обозначим \(\xi_1 \dots \xi_n\) за расставленные по возрастанию величины \(\eta_{n + 1} \dots \eta_{2n - 1}, \xi_1 = 0, \xi_n = 1\). Тогда случайные величины
    \[\mu_i = - \frac{1}{\alpha}(\xi_i - \xi_{i + 1})\ln(\eta_1 \dots \eta_n), 1 \leq i \leq n\]
    независимы и имеют показательное распределение с параметром \(\alpha\).
\end{theorem}
\begin{proof}
    Не будем.
\end{proof}

\begin{example}
    При \(n = 5\), пусть \(\eta_5 > \eta_4\). Тогда:
    \begin{align*}
        \mu_1 & = - \frac{1}{\alpha}\eta_4 \ln (\eta_1\eta_2\eta_3)            \\
        \mu_2 & = - \frac{1}{\alpha}(\eta_5 - \eta_4) \ln (\eta_1\eta_2\eta_3) \\
        \mu_3 & = - \frac{1}{\alpha}(1 - \eta_5) \ln (\eta_1\eta_2\eta_3)      \\
    \end{align*}
\end{example}

Преимущество по сравнению с методом обратной функции --- нужно вычислять только один логарифм. Минус --- требуется сортировка. Способ наиболее эффективен при \(n = 3\), при этом он примерно вдвое эффективнее метода обратной функции.
%</53>

\subsection{Моделирование дискретных распределений}

\subsubsection{Общий метод}

%<*51.2>
Пусть имеется дискретное распределение, \(p_k = P(\xi = c_k), k = 1, 2, \dots\). \(r_0 \coloneqq 0, r_m = \sum_{k=1}^{m} p_k\) --- концы отрезков разбиения.

Пусть \(y_i \in U(0, 1)\) --- псевдослучайное число. Если \(y_i \in [r_{i-1}, r_i)\), то \(x_i = c_i\). На самом деле это тот же метод обратной функции. В частности, если моделируем распределение Бернулли \(B_p\), тогда \(x_i = \begin{cases}
    0, & y_i \in [0, 1 - p) \\
    1, & y_i \in [1 - p, 1] \\
\end{cases}\)
%</51.2>

%<*54>
\subsubsection{Биномиальное распределение}

\[P(\xi = k) = C_n^k p^k q^{n - k} \quad 0 \leq k \leq n\]
Смысл: число успехов при \(n\) испытаниях. Будем на это опираться.

Берём \(n\) значений датчика \(y_i, 1 \leq i \leq n\) и положим \(z_i \coloneqq \begin{cases}
    0, & y_i \in [0, 1 - p) \\
    1, & y_i \in [1 - p, 1]
\end{cases}\). Тогда \(x = \sum_{i=1}^{n} z_i\) --- смоделированное значение \(\in B_{n, p}\)

\subsubsection{Геометрическое распределение}

\[P(\xi = k) = (1 - p)^{k-1}p\]
Смысл: номер первого успешного испытания.

Берём последовательные значения датчика \(y_i\) до тех пор, пока \(y_i \in [1 - p, p]\). Ответ --- индекс последнего опыта.

\subsubsection{Распределение Пуассона}

\[P(\xi = k) = \frac{\lambda^k}{k!} e^{ - \lambda}, k \geq 0\]
\begin{theorem}
    Пусть \(\mu_1, \mu_2 \dots \in E_\alpha\) --- независимые случайные величины. Положим \(S_n = \mu_1 + \dots + \mu_n, N = \max \{k \mid S_k \in [0, 1]\}\). Тогда \(N \in \Pi_\lambda\).
\end{theorem}

На основе теоремы и метода обратных функций получаем формулу для моделирования:
\[x_i = \min \left\{x \mathrel{\Big|} \prod_{j=1}^{n} y_{ij} < e^{ - \lambda}\right\}\]
%</54>
