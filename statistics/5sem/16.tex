\chapter{20 декабря.}

\section{Энтропия}

Пусть \(\xi\) --- результат некоторого эксперимента с возможными исходами \(A_1 \dots A_N\), вероятности которых \(p_1 \dots p_N\) соответственно.

\begin{definition}
    \textbf{Энтропией эксперимента} называется величина
    \[H(\xi) \coloneqq - \sum_{i=1}^{N} p_i \log_2 p_i\]
\end{definition}

\begin{remark}
    При \(p_i = 0\) полагаем \(p_i \log_2 p_i = 0\), т.к. \(\lim_{x \to + 0} x \log_2 x = 0\).
\end{remark}

\begin{prop}\itemfix
    \begin{itemize}
        \item \(H(\xi) \geq 0\), т.к. \(\log_2 p_i \leq 0\), т.к. \(p_i < 1\).
        \item \(H(\xi) = 0 \Leftrightarrow \exists i : p_i = 1\), т.е. результат эксперимента предопределен.
        \item Максимум энтропии, равный \(H_0 = \log_2 N\), достигается \(p_1 = \dots p_N = \frac{1}{N}\).
              \begin{proof}
                  Рассмотрим функцию \(y = x \cdot \log_2 x\). При \(x > 0\)
                  \[y'' = \left(\log_2 x + \frac{1}{\ln 2}\right)' = \frac{1}{x \ln 2} > 0\]
                  Следовательно, функция \(\varphi(x)\) выпукла вниз.

                  Рассмотрим дискретную случайную величину \(\eta\) с \(N\) равновероятными исходами \(p_1 \dots p_N\). Тогда по неравенству Йенсена:
                  \begin{align*}
                      \varphi(\E \eta)
                       & = \varphi \left(\frac{1}{N} \sum_{i=1}^{N} p_i\right) \\
                       & = \varphi (\frac{1}{N})                               \\
                       & = - \frac{1}{N} \log_2 N                              \\
                       & \leq \E \varphi(n)                                    \\
                       & = \sum_{i=1}^{N} \frac{1}{N} p_i \log_2 p_i           \\
                       & = \frac{1}{N} \sum_{i=1}^{N} p_i \log_2 p_i
                  \end{align*}
                  \begin{align*}
                      - \frac{1}{N} \log_2 N & \leq \frac{1}{N} \sum_{i=1}^{N} p_i \log_2 p_i \\
                      \log_2 N               & \geq H
                  \end{align*}
              \end{proof}
    \end{itemize}
\end{prop}

Вывод: \(H\) можно рассматривать как количественную характеристику меры неопределенности эксперимента.
\begin{itemize}
    \item Если \(H = H_{\min} = 0\), то результат предопределен.
    \item Если \(H = H_{\max} = \log_2 N\), то неопределенность максимальна.
\end{itemize}

\begin{example}
    \(\xi \in B_p, H(\xi) = - p \log_2 p - (1 - p) \log_2(1 - p)\). Максимальная энтропия при \(p = \frac{1}{2}\).
\end{example}

Оказалось, что таким образом определенная энтропия является числовой характеристикой реальных процессов природы и техники, связанных с передачей информации.

\begin{example}\itemfix
    \begin{enumerate}
        \item Время реакции человека на (одну из \(N\)) загоревшуюся лампочку прямо пропорционально \(\log_2 N\).
        \item Понятие энтропии пришло из физики.

              Второй закон термодинамики: энтропия замкнутой системы возрастает.

        \item Теория кодирования информации.

              Для кодирования каждого из \(N\) символов нужно \(\log_2 N\) бит.

              \textcolor{red}{Часть лекции про теоркод опущена.}ы
    \end{enumerate}
\end{example}

\subsection{Непрерывные распределения с наибольшей энтропией}

\begin{definition}
    Пусть \(\xi\) --- абсолютно непрерывная случайная величина с плотностью \(f(x)\) и носителем \(A = \{x \mid f(x) > 0\}\). Тогда \textbf{энтропией} называется величина
    \[H(\xi) = - \int_A f(x) \log_2 f(x) dx\]
\end{definition}

Следующие распределения имеет наибольшую энтропию:
\begin{enumerate}
    \item Если \(A = (0; 1)\), то \(U(0; 1)\)
    \item Если \(A = (0; \infty)\) и \(\E \xi = 1\), то \(E_1\) (показательное распределение)
    \item Если \(A = ( - \infty, \infty), \E \xi = 0, \D \xi = 1\), то \(N(0, 1)\)
\end{enumerate}

\section{Задача о \?}

Пусть \(q = 1 - p\) --- вероятность проигрыша. Ставка \(1\). Пусть у первого игрока напитал \(k\), у второго --- \(m - k\). Игра прекращается, когда один из игроков теряет весь капитал. Найти вероятность разорения первого игрока.

Пусть \(r_k\) --- вероятность разорения первого игрока при капитале \(k\). Разложим её по результатам первого шага: \(r_k = p r_{k+1} + q r_{k-1}\).
\[pr_{k+1} - r_k + (1 - p)r_{k-1} = 0 \quad r_0 = 1 \quad r_m = 0\]
\[p\lambda^2 - \lambda + (1 - p) = 0\]
\unfinished
