\chapter{25 октября}

\section{Статистическая зависимость}

\begin{definition}
    \textbf{Функциональная зависимость} имеет место, когда две величины связаны жесткими законами природы.
\end{definition}

\begin{definition}
    Зависимость называется \textbf{статистической}, если изменение одной величины влияет на распределение другой. Если при этом изменяется среднее значение\footnote{Математическое ожидание.} другой случайной величины, то зависимость называется \textbf{корреляционной}. Если среднее значение \textit{увеличивается} при увеличении первой случайной величины, то корреляция \textbf{прямая}, а если \textit{уменьшается} --- \textbf{обратная}.
\end{definition}

\subsection{Корреляционное облако}

Пусть в ходе экспериментов получились значения случайных величин \(X\) и \(Y\) : \((X_i, Y_i), 1 \leq i \leq n\). Нанося эти точки  на координатную плоскость \(XOY\), получим корреляционное облако. По его виду можно сделать предположение о зависимости.

\begin{example}
    \(X, Y\) имеют нормальное распределение с одинаковыми параметрами.
    \begin{itemize}
        \item Если корреляционное облако имеет форму круга, то величины \textit{независимы}.
        \item  Если корреляционное облако имеет форму эллипса c большой осью параллельной прямой вида \(y = kx + b, k > 0\), то скорее всего \textit{зависимость прямая}.
    \end{itemize}
\end{example}

\subsection{Корреляционная таблица}

Экспериментальные данные представляем в виде таблицы:

\begin{center}
    \begin{tabular}{CCCCC}
        \toprule
        \text{\diagbox{\(X_i\)}{\(Y_i\)}} & Y_1    & Y_2    & \dots  & Y_m    \\ \midrule
        X_1                               & n_{11} & n_{12} & \dots  & n_{1m} \\
        X_2                               & n_{21} & n_{22} & \dots  & n_{2m} \\
        \vdots                            & \vdots & \vdots & \ddots & \vdots \\
        X_n                               & n_{n1} & n_{n2} & \dots  & n_{nm} \\
        \bottomrule
    \end{tabular}
\end{center}

\begin{example}\itemfix
    \begin{center}
        \begin{tabular}{CCCCCCC}
            \toprule
            \text{\diagbox{\(X_i\)}{\(Y_i\)}} & 10 & 20 & 30 & 40 & n_x & \overline{y_x} \\ \midrule
            2                                 & 7  & 3  & 0  & 0  & 10  & 13             \\
            4                                 & 3  & 10 & 10 & 2  & 25  & 24.4           \\
            6                                 & 0  & 2  & 10 & 3  & 15  & 30.67          \\
            n_y                               & 10 & 15 & 20 & 5  & 50  &                \\
            \bottomrule
        \end{tabular}
    \end{center}

    \begin{itemize}
        \item \(n_x\) --- частота значения \(x\)
        \item \(n_y\) --- частота значения \(y\)
        \item \(\overline{y_x}\) --- условное среднее случайной величины \(y\):
              \[\overline{y_x} = \frac{1}{n_x} \sum_i n_{xy_i} y_i\]
    \end{itemize}

    В нашем примере условное матожидание \(\overline{y_x}\) увеличивается при увеличении \(x\), следовательно скорее всего есть прямая корреляция.
\end{example}

\begin{remark}
    При большом числе данных удобнее составлять интервальную корреляционную таблицу и заменить интервалы на среднее.
\end{remark}

\subsection{Критерий \(\chi^2\) для проверки независимости}

Пусть выборка \((X_1, Y_1) \dots (X_n, Y_n)\) случайных величин \(X\) и \(Y\) представлена в виде интервальной корреляционной таблицы. Случайная величина \(X\) при этом разбита на \(k\) интервалов, а \(Y\) на \(m\) интервалов. Обозначим \(v_{i.} =\) число значений случайной величины \(X\), попавших в \(i\)-тый интервал \([a_{i-1}, a_i), 1 \leq i \leq k\). Обозначим \(v_{.j} =\) число значений случайной величины \(Y\), попавших в \(j\)-тый интервал \([b_{j-1}, b_j), 1 \leq j \leq k\). Обозначим \(v_{ij} =\) число точек \((X, Y)\), попавших в \([a_{i-1}, a_i) \times [b_{i-1}, b_i)\).

\begin{center}
    \begin{tabular}{CCCCCC}
        \toprule
        \text{\diagbox{\(X_i\)}{\(Y_i\)}}
                                  & [b_0; b_1) & [b_1; b_2) & \dots  & [b_{m-1}; b_m) & v_i = \sum_{j=1}^n v_{ij} \\ \midrule
        {}[a_0; a_1)              & v_{11}     & v_{12}     & \dots  & v_{1m}         & v_{1.}                    \\
        {}[a_1; a_2)              & v_{21}     & v_{22}     & \dots  & v_{2m}         & v_{2.}                    \\
        \vdots                    & \vdots     & \vdots     & \ddots & \vdots                                     \\
        {}[a_{n-1}; a_n)          & v_{n1}     & v_{n2}     & \dots  & v_{nm}         & v_{n.}                    \\
        v_j = \sum_{i=1}^n v_{ij} & v_{.1}     & v_{.2}     & \dots  & v_{.m}                                     \\
        \bottomrule
    \end{tabular}
\end{center}

По этой таблице проверяется основная гипотеза \(H_0 : X\) и \(Y\) независимы против \(H_1 : X\) и \(Y\) зависимы.

Вспомним определение независимых случайных величин:
\[P(X \in \mathfrak{B}_1, Y \in \mathfrak{B}_2) = P(X \in \mathfrak{B}_1) \cdot P(Y \in \mathfrak{B}_2)\]
Согласно этому определению, если гипотеза \(H_0\) верна, то вероятность попадания пары \((X, Y)\) в любой прямоугольник равна произведению теоретических вероятностей попасть случайным величинам в эти интервалы.
\[p_{ij} = P (X \in [a_{i-1}; a_i), Y \in [b_{i-1}; b_i)) = P(X \in [a_{i-1}; a_i)) \cdot P(Y \in [b_{i-1}; b_i)) = p_i \cdot p_y\]

По закону больших чисел при \(n \to \infty\):
\[\frac{v_{i.}}{n} \xrightarrow{P} p_i \quad \frac{v_{.j}}{n} \xrightarrow{P} p_j \quad \frac{v_{ij}}{n} \xrightarrow{P} p_{ij}\]

Поэтому основанием для отклонения гипотезы служит заметная разница между величинами между \(\frac{v_{ij}}{n}\) и \(\frac{v_i}{n} \cdot \frac{v_j}{n}\), т.е. между \(v_{ij}\) и \(\frac{v_iv_j}{n}\).

В качестве статистики критерия берётся функция:
\[K = n \sum_{i, j} \frac{\left(v_{ij} - \frac{v_iv_j}{n}\right)^2}{v_{i.}v_{.j}}\]

\begin{theorem}
    Если гипотеза \(H_0\) верна, то \(K \xrightrightarrows{} \chi^2_{(k - 1)(m - 1)}\)
\end{theorem}

Получили критерий согласия: \(t_k\) --- квантиль распределения \(H_{(k - 1)(m - 1)}\)
\[\begin{cases}
        H_0, & K < t_k    \\
        H_1, & K \geq t_k
    \end{cases}\]

\subsection{Однофакторный дисперсионный анализ}

Предположим, что на случайную величину \(X\) \textit{(признак--результат)} может влиять фактор \(Z\) \textit{(признак--фактор)}. \(Z\) --- не обязательно случайная величина.

\begin{example}
    Хотим проверить, как влияет температура на разложение \?. Проводим измерения при разных температурах, регулируя термостат. Тогда температура --- не случайная величина, она управляема.
\end{example}

Пусть при различных \(k\) уровнях фактора \(Z\) получены \(k\) независимых выборок случайной величины \(X : X^{(1)} = (X^{(1)}_1 \dots X^{(1)}_{n_1}) \dots X^{(k)} = (X^{(k)}_1 \dots X^{(k)}_{n_k})\). В общем случае размеры выборок могут быть различны.

\subsubsection{Общая, межгрупповая и внутригрупповая дисперсия}

\[\overline{X}^{(j)} = \frac{1}{n_j} \sum_{i=1}^{n_j} X^{(j)}_i \quad \D^{(j)} = \frac{1}{n_j} \sum_{i=1}^{n_j} (X_i^{(j)} - \overline{X}^{(j)})^2\]

Объединив все данные в одну общую выборку, получим выборку объёма \(n = n_1 + \dots + n_k\). Вычислим общее выборочное среднее как
\[\overline{X} = \frac{1}{n} \sum_{i,j} X_i^{(j)} = \frac{1}{n} \sum_{j=1}^{k} \overline{X}^{(j)}\]
и \textbf{общую выборочную дисперсию}:
\[D_o = \frac{1}{n} \sum_{i, j} (X_i^{(j)} - \overline{X})^2\]

\begin{definition}
    \textbf{Внутригрупповой} \textit{(остаточной)} дисперсией называется среднее\footnote{взвешенное} групповых дисперсий:
    \[D_{\mathrm{в}} = \frac{1}{n} \sum_{j=1}^{k} \D^{(j)} n_j\]
\end{definition}

\begin{definition}
    \textbf{Межгрупповой} \textit{(факторной)} дисперсией или \textbf{дисперсией выборочных средних} называется величина
    \[D_{\mathrm{м}} = \frac{1}{n} \sum_{j=1}^{k} (\overline{X}^{(j)} - \overline{X})^2 n_j\]
\end{definition}

\begin{theorem}[о разложении дисперсий]
    \label{разложение дисперсий}
    Общая дисперсия равна сумме межгрупповой и внутригрупповой дисперсий:
    \[D_o = D_{\mathrm{в}} + D_{\mathrm{м}}\]
\end{theorem}
\begin{proof}
    Неинтересное, алгебраическое.
\end{proof}

\textit{Смысл:}
\begin{itemize}
    \item Внутригрупповая дисперсия показывает средний разброс внутри выборок.
    \item Межгрупповая дисперсия показывает, насколько отличны выборочные средние при различных уровнях фактора. Таким образом, её величина в общей сумме отражает влияние фактора.
\end{itemize}

\subsubsection{Проверка гипотезы о влиянии фактора}

Предположим, что случайна величина \(X\) имеет нормальное распределение и фактор \(Z\) может влиять только на математическое ожидание, но не на дисперсию и тип распределения.

Может показаться, что ограничение слишком строгое, но в реальной жизни это условие выполняется часто.

Поэтому можно считать, что данные независимые выборки при разных уровнях \(Z\) также имеют нормальное распределение с одинаковым параметром \(\sigma^2\):
\[X^{(j)}_i \in N(a_i, \sigma^2)\]
Проверяется основная гипотеза \(H_0 : a_1 = a_2 = \dots = a_k\), т.е. фактор \(Z\) не влияет на \(X\). \(H_1 : Z\) влияет на \(X\). По пункту 3 основной теоремы:
\[\sum_{i=1}^{n} \left(\frac{X_i - \overline{X}}{\sigma}\right)^2 = \frac{n D_{\mathrm{в}}}{S^2} \in H_{n-1} \]
Отсюда для каждой из \(k\) выборок:
\[\frac{n_j \D^{(j)}}{\sigma} \in H_{n_j - 1}, 1 \leq j \leq k\]
Т.к. распределение \(\chi^2\) устойчиво по суммированию, то получаем:
\[\sum_{j=1}^{k} \frac{n_j \D^{(j)}}{\sigma} = \frac{\sum_{j=1}^{k} n_j \D^{(j)}}{\sigma^2} = \frac{n D_{\mathrm{в}}}{\sigma^2} \in H_{n - k}\]
, т.к. \(\sum_{j=1}^{k} (n_j - 1) = n - k\).

\blfootnote{На записи все эти формулы видны примерно как ``Ыаыаыаацпы''} % допустим, пасхалка? не знаю, пристрелите меня

Все это выполнено вне зависимости от того, верна \(H_0\) или нет. Пусть \(H_0\) верна, тогда все выборки можно считать одной выборкой объёма и по тому же свойству:
\[\frac{nD_o}{\sigma^2} \in H_{n-1}\]
Согласно теореме о разложении дисперсии:
\[\underbrace{\frac{nD_o}{\sigma^2}}_{\in H_{n-1}} = \frac{nD_{\mathrm{в}}}{\sigma^2} + \underbrace{\frac{nD_{\mathrm{м}}}{\sigma^2}}_{\in H_{n - k}}\]

Следовательно, \(\frac{nD_{\mathrm{в}}}{\sigma^2} \in H_{k-1}\).\footnote{Это неочевидный факт, но мы его доказывать не будем.}. В итоге при верной гипотезе \(H_0\) мы получили \(\frac{nD_{\mathrm{в}}}{\sigma^2} \in H_{k-1}\), а \(\frac{nD_{\mathrm{м}}}{\sigma^2} \in H_{n - k}\). Тогда:
\[\cfrac{\cfrac{nD_{\mathrm{в}}}{\sigma^2(k - 1)}}{\cfrac{nD_{\mathrm{м}}}{\sigma^2(n - k)}} = \cfrac{\cfrac{D_{\mathrm{в}}}{k - 1}}{\cfrac{D_{\mathrm{м}}}{n - k}} \in F(k - 1, n - k)\]
В результате имеем критерий \(K = \frac{n - k}{n - 1}\frac{D_{\mathrm{в}}}{D_{\mathrm{м}}}\), находим \(t_k\) --- квантиль \(F(k - 1, n - k)\) уровня значимости \(\alpha\), искомое очевидно строится.
