\chapter{2 сентября}

\section{Введение}

\begin{definition}
    \textbf{Передаваемый сигнал} это
    \[x(t) = \sum_i S_{x_i} (t - iT)\]
    , где \(x_i\) --- передаваемые символы, \(T\) --- продолжительность передачи одного символа.
\end{definition}

\begin{example}[\(M\)-ичная амплитудно-импульсная модуляция]
    \[S_i(t) = \alpha(2i + 1 - M)g(t) \sin(2 \pi ft)\]
    , где:
    \begin{itemize}
        \item \(g(t)\) --- сигнальный импульс
        \item \(f\) --- несущая частота
        \item \(\alpha\) --- коэффициент энергии передаваемого сигнала
    \end{itemize}
\end{example}

Модели канала:
\begin{enumerate}
    \item В непрерывном времени: \(y(t) = x(t) + \eta(t)\)
    \item В дискретном времени: \(y_i = \alpha(2 x_i + 1 - M) + \eta_i\)
\end{enumerate}

\(\eta\) --- белый шум, обычно Гауссов \(\mathcal{N}(0, \sigma^2)\).

У передаваемого сигнала обычно не должно быть постоянной компоненты (по причинам физики), поэтому сигнал симметричен.
С точки зрения теории кодирования это несущественно.

Приемник наблюдает на выходе канала вектор \(y = (y_0 \ldots y_{n-1})\).
Канал характеризуется условным распределением \(P_{Y \mid X}(y \mid x)\),
где \(X, Y\) --- случайные величины, соответствующие векторам переданных и принятых символов.

Приемник разбивает векторное пространство на решающие области \(R_x : y \in R_x \implies \hat{x} = x\), т.е. если сигнал попал в область \(R_x\), то мы ему сопоставляем кодовый символ \(x\).
Тогда вероятность ошибки:
\begin{align}
    P_e & = \int_{\R^N} P_e(y) p_Y(y) dy                               \\
        & = \sum_x \int_{R_x} p_e(y) p_Y(y) dy                         \\
        & = \sum_x \int_{R_x} (1 - p_{X \mid Y}\{x \mid y\}) p_Y(y) dy \\
        & = 1 - \sum_x \int_{R_x} p_{X \mid Y} \{x \mid y\} p_Y(y) dy
\end{align}

Приемник должен найти оптимальные решающие области.
Оптимальность определяется критерием, например:
\begin{enumerate}
    \item Критерий максимума апостериорной вероятности (критерий идеального наблюдателя)
          \begin{align*}
              R_x
               & = \{y \mid p_{X \mid Y}(x \mid y) > p_{X \mid Y} (x' \mid y), x ' \neq x\}               \\
               & = \{y \mid P_X(x) p_{Y \mid X}(y \mid x) > P_X(x') p_{Y \mid X} (y \mid x'), x' \neq x\}
          \end{align*}
    \item Критерий максимума правдоподобия
          \[R_x = \{y \mid p_{Y \mid X}(y \mid x) > p_{Y \mid X}(y \mid x'), x' \neq x\}\]
          В случае равновероятных символов этот критерий совпадает с критерием идеального наблюдателя.
\end{enumerate}

\begin{example}[2-ичная амплитудно-импульсная модуляция (2-АМ)]
    Пусть \(y_i = \alpha(2x_i - 1) + \eta_i, \eta_i \sim \mathcal{N}(0, \sigma^2), x_i \in \{0, 1\} \). Тогда:
    \[p_{Y \mid X}(y \mid x)
        = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{- \frac{(y - \alpha(2x - 1))^2}{2 \sigma^2}}\]

    Применим критерий максимального правдоподобия:
    \[R_0 = \{y \mid y < 0\}, R_1 = \{y \mid y \ge 0\} \]

    Вычислим вероятность ошибки:
    \begin{align*}
        P_e & = P_X(0)P \{Y \ge 0 \mid X = 0\} + P_X(1) P \{Y < 0 \mid X = 1\}                               \\
            & = 0.5 \int_0^\infty p_{Y \mid X}(y \mid 0) dy + 0.5 \int_{-\infty}^0 p_{Y \mid X}(y \mid 1) dy \\
            & = \int_0^\infty \frac{1}{\sqrt{2 \pi \sigma^2}} e^{ - \frac{(y + \alpha)^2}{2\sigma^2}} dy     \\
            & = \int_\alpha^\infty \frac{1}{\sqrt{2 \pi \sigma^2}} e^{ - \frac{y^2}{2\sigma^2}} dy           \\
            & = \int_{\frac{\alpha}{\sigma}}^\infty \frac{1}{\sqrt{2 \pi}} e^{ - \frac{y^2}{2}} dy           \\
            & \eqqcolon Q \left(\frac{\alpha}{\sigma}\right)                                                 \\
            & = \frac{1}{2} \text{erfc} \left(\frac{\alpha}{\sqrt{2}\sigma}\right)
    \end{align*}
\end{example}

Значение сигнала это обычно уровень напряжения.
Как мы знаем из школьной физики, мощность \(P = \frac{U^2}{R}\).
Мы хотим минимизировать мощность, чтобы экономить электроэнергию.
Мощность сигнала суть случайная величина с матожиданием, пропорциональным \(E_S = \alpha^2\).
Мощность белого шума не зависит от частоты и пропорциональна \(\sigma^2 = \frac{N_0}{2}\).
Если же шум зависит от частоты, то он называется розовым или голубым.

Соотношение мощностей сигнал/шум на символ это \(\frac{E_S}{N_0}\), обычно измеряемое в децибелах, т.е. \(10 \log_{10} \frac{E_S}{N_0}\).
Однако нас интересуют не символы, а биты и тогда соотношение сигнал/шум на бит это \(\frac{E_S}{RN_0}\),
где \(R\) --- количество бит информации, представленных одним символом.

\subsection{Способы снижения вероятности ошибки}

\begin{enumerate}
    \item Посимвольное повторение: будем передавать вместо каждого символа \(m\) копий того же символа.
          \[y_{mi + j} = \alpha (2x_i - 1) + \eta_{mi + j}, \quad 0 \le j < m\]
          Рассмотрим разные способы работы приемника:
          \begin{itemize}
              \item  Для каждого \(y_{mi + j}\) проведем голосование. Тогда вероятность ошибки:
                    \[P_v(m) = \sum_{j = \left\lceil \frac{m}{2} \right\rceil }^{m - 1} C_m^j P_e^j (1 - P_e)^{m - j}\]
              \item Примем решение по
                    \[\sum_{j = 0}^{m - 1} y_{mi + j} = m \alpha(2 x_i - 1) + \sum_{j = 0}^{m - 1} \eta_{mi + j}\]
                    , т.е сложим наблюдения в одном блоке. Тогда вероятность ошибки:
                    \[P_a(m) = Q\left(\frac{m \alpha}{\sqrt{m} \sigma }\right) = Q \left( \sqrt{2 \frac{m E_s}{N_0}} \right) = Q \left( \sqrt{2 \frac{E_b}{N_0}} \right)\]
          \end{itemize}
          Выигрыша не будет, т.к. на один символ передается в \(m\) раз меньше бит (см. последний переход).

          Второй метод лучше первого, т.к. мы не теряем информацию о нашей уверенности в каждом принятом символе.
\end{enumerate}

Избыточность на уровне битов не дала улучшения, поэтому введем избыточность на уровне блоков.

\subsection{Понятие кода}

\begin{definition}
    \textbf{Код} --- множество допустимых последовательностей символов алфавита \(X\).
\end{definition}

Последовательности могут быть конечными или бесконечными, но на практике только конечными. Не каждая последовательность символов алфавита является кодовой.

\begin{definition}
    \textbf{Кодер} --- устройство, отображающее информационные последовательности символов алфавита \(\mathcal B\) в кодовые.
\end{definition}

Различным последовательностям алфавита \(\mathcal B\) сопоставляются различные последовательности алфавита \(X\) для однозначности кодирования.

\begin{definition}
    \textbf{Скорость кода} --- отношение длин информационной и кодовой последовательностей.
\end{definition}

\begin{definition}
    \textbf{Декодер} --- устройство, восстанавливающее по принятой последовательности символов \underline{наиболее вероятную} кодовую последовательность.
\end{definition}

\subsection{Теоремы кодирования}

Пусть для передачи используется код \(\mathcal C \subset X^n\) длины \(n\), состоящий из \(M\) кодовых слов, выбираемых с одинаковой вероятностью.

\begin{theorem}[обратная]
    Для дискретного постоянного канала с пропускной способностью \(C\) для любого \(\delta > 0\) существует \(\varepsilon > 0\) такое, что для любого кода со скоростью \(R > C + \delta\) средняя вероятность ошибки \(\overline{P_e} \ge \varepsilon\)
\end{theorem}

\begin{theorem}[прямая]
    Для дискретного постоянного канала с пропускной способностью \(C\) для любых \(\varepsilon, \delta > 0\) существует достаточно большое число \(n_0 > 0\), такое что для всех натуральных \(n \ge n_0\) существует код длиной \(n\) со скоростью \(R \ge C - \delta\), средняя вероятность ошибки которого \(\overline{P_e} \le \varepsilon\)
\end{theorem}

\subsection{Пропускная способность некоторых каналов}

\begin{enumerate}
    \item Двоичный симметриный канал: \(X, Y \in \{0, 1\}, p_{Y \mid X}(y \mid x) = \begin{cases}
              p,      & y \neq x \\
              1 - p , & y = x
          \end{cases}\)
          \[C_{BSC} = 1 + p \log_2 p + (1 - p) \log_2(1 - p)\]
    \item Идеальный частотно ограниченный гауссовский канал: \(y(t) = x(t) + \eta(t), \eta(t)\) --- гауссовский случайный процесс, спектральная плотность мощности которого равна \(S(f) = \begin{cases}
              \frac{N_0}{2}, & -W < f < W   \\\
              0,             & \text{иначе}
          \end{cases}\)
          Из теории случайных процессов:
          \[C_{AWGN} = W \log_2 \left(1 + \frac{E_S}{WN_0}\right)\]
          \[\lim_{W \to \infty} C_{AWGN} = \frac{E_S}{N_0 \ln 2}\]
\end{enumerate}
