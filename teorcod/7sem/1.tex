\chapter{2 сентября}

\section{Введение}

\begin{definition}
    \textbf{Передаваемый сигнал} это
    \[x(t) = \sum_i S_{x_i} (t - iT)\]
    , где \(x_i\) --- передаваемые символы, \(T\) --- продолжительность передачи одного символа.
\end{definition}

\begin{example}[\(M\)-ичная амплитудно-импульсная модуляция]
    \[S_i(t) = \alpha(2i + 1 - M)g(t) \sin(2 \pi ft)\] 
    , где:
    \begin{itemize}
        \item \(g(t)\) --- сигнальный импульс
        \item \(f\) --- несущая частота
        \item \(\alpha\) --- коэффициент энергии передаваемого сигнала
    \end{itemize}
\end{example}

Модели канала:
\begin{enumerate}
    \item В непрерывном времени: \(y(t) = x(t) + \eta(t)\)
    \item В дискретном времени: \(y_i = \alpha(2 x_i + 1 - M) + \eta_i\)
\end{enumerate}

\(\eta\) --- белый шум, обычно Гауссов \(\mathcal{N}(0, \sigma^2)\).

У передаваемого сигнала обычно не должно быть постоянной компоненты, поэтому сигнал симметричен.
С точки зрения теории кодирования это несущестувенно.

Приемник наблюдает на выходе канала вектор \(y = (y_0 \ldots y_{n-1})\).
Канал характеризуется условным распределением \(P_{Y \mid X}(y \mid x)\),
где \(X, Y\) --- случайные величины, соответствующие векторам переданных и принятых символов.

Приемник разбивает векторное пространство на решающие области \(R_x : y \in R_x \implies \hat{x} = x\).
Тогда вероятность ошибки:
\[P_e = \int_{\R^N} P_e(y) p_Y(y) dy = \sum_x \int_{R_x} ? = 1 - \sum_x \int_{R_x} p_{X \mid Y} \{x \mid y\} p_Y(y) dy\] 

Приемник должен найти оптимальные решающие области.
Оптимальность определяется критерием, напримрер:
\begin{enumerate}
    \item Критерий максимума апостериорной вероятности (критерий идеального наблюдаетля)
        \begin{align*}
            R_x
            & = \{y \mid p_{X \mid Y}(x \mid y) > p_{X \mid Y} (x' \mid y), x ' \neq x\} \\ 
            & = \{y \mid P_X(x) p_{Y \mid X}(y \mid x) > P_x(x') p_{Y \mid X} (y \mid x'), x' \neq x\}  
        \end{align*}
    \item Критерий максимума правдоподобия
        \[R_x = \{y \mid p_{Y \mid X} \?\} \] 
        В случае равновероятных символов этот критерий совпадает с критерием идеального наблюдателя.
\end{enumerate}

\begin{example}[2-ичная амплитудно-импульсная модуляция (2-АМ)]
    Пусть \(y_i = \alpha(2x_i - 1) + \eta_i, \eta_i \sim \mathcal{N}(0, \sigma^2), x_i \in \{0, 1\} \). Тогда:
    \[p_{Y \mid X}(y \mid x)
    = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{- \cfrac{(y - \alpha(2x - 1))^2}{2 \sigma^2}}\] 

    Применим критерий максимального правдоподобия:
    \[R_0 = \{y \mid y < 0\}, R_1 = \{y \mid y \ge 0\} \] 

    Вычислим вероятность ошибки:
    \begin{align*}
        P_e & = P_X(0)P \{Y \ge 0 \mid X = 0\} + P_X(1) P \{Y < 0 \mid X = 1\} \\
         & = 0.5 \?
    \end{align*}
\end{example}

Значение сигнала это обычно уровень напряжения. 
Как мы знаем из школьной физики, мощность \(P = \frac{U^2}{R}\).
Мы хотим минимизировать мощность, чтобы экономить электроэнергию.
Мощность сигнала суть случайная величина с матожиданием, пропорциональным \(E_S = \alpha^2\).
Мощность белого шума не зависит от частоты и пропорциональна \(\sigma^2 = \frac{N_0}{2}\).
Если же шум зависит от частоты, то он называется розовым или голубым.

Соотношение мощностей сигнал/шум на символ это \(\frac{E_S}{N_0}\), обычно измеряемое в децибелах, т.е. \(10 \log_{10} \frac{E_S}{N_0}\).
Однако, нас интересуют не символы, а биты и тогда соотношение сигнал/шум на бит это \(\frac{E_S}{RN_0}\),
где \(R\) --- количество бит информации, представленных одним символом.

\subsection{Способы снижения вероятности ошибки}

\begin{enumerate}
    \item Посимвольное повторение: будем передавать вместо каждого символа \(m\) копий того же символа.
        \[y_{mi + j} = \alpha (2x_i - 1) + \eta_{mi + j}, \quad 0 \le j < m\] 
        Рассмотрим разные способы работы приемника:
        \begin{itemize}
            \item  Для каждого \(y_{mi + j}\) проведем голосование. Тогда вероятность ошибки:
        \[P_v(m) = \sum_{j = \left\lceil \frac{m}{2} \right\rceil }^{m - 1} C_m^j P_e^j (1 - P_e)^{m - j}\] 
            \item Примем решение по
                \[\sum_{j = 0}^{m - 1} y_{mi + j} = m \alpha(2 x_i - 1) + \sum_{j = 0}^{m - 1} \eta_{mi + j}\] 
                , т.е сложим наблюдения в одном блоке. Тогда вероятность ошибки:
                \[P_a(m) = Q\left(\frac{m \alpha}{\sqrt{m} \sigma }\right) = Q \left( \sqrt{2 \frac{m E_s}{N_0}} \right) = Q \left( \sqrt{2 \frac{E_b}{N_0}} \right)\] 
        \end{itemize}
        Выигрыша не будет, т.к. на один символ передается в \(m\) раз меньше бит (см. последний переход).

        Второй метод лучше первого, т.к. мы не теряем информацию о нашей уверенности в каждом принятом символе.
\end{enumerate}

Избыточность на уровне битов не дала улучшения, поэтому введем избыточность на уровне блоков.

\subsection{Понятие кода}

\begin{definition}
    \textbf{Код} --- множество допустимых последовательностей символов алфавита \(X\).
\end{definition}

Последовательности могут быть конечными или бесконечными, но на практике только конечными. Не каждая последовательность символов алфавита является кодовой.

\begin{definition}
    \textbf{Кодер} --- устройство, отображающее информационные последовательности символов алфавита \(\mathcal B\) в кодовые.
\end{definition}

Различным последовательностям алфавита \(\mathcal B\) сопоставляются различные последовательности алфавита \(X\).
\?

\subsection{Теоремы кодирования}

Пусть для передачи используется код \(\mathcal C \subset X^n\) длины \(n\), состоящий из \(M\) кодовых слов, выбираемых с одинаковой вероятностью.

\begin{theorem}[обратная]
    Для дискретного постоянного канала с пропускной способностью \(C\) для любого \(\delta > 0\) существует \(\varepsilon > 0\) такое, что для любого кода со скоростью \(R > C + \delta\) средняя вероятность ошибки \(\overline{P_e} \ge \varepsilon\)
\end{theorem}

\begin{theorem}[прямая]
    Для дискретного постоянного канала с пропускной способностью \(C\) для любых \(\varepsilon, \delta > 0\) существует достаточно большое число \(n_0 > 0\), такое что для всех натуральных \(n \ge n_0\) существует код длиной \(n\) со скоростью \(R \ge C - \delta\), среднеяя вероятность ошибки которого \(\overline{P_e} \le \varepsilon\)
\end{theorem}

\subsection{Пропускная способность некоторых каналов}

\begin{enumerate}
    \item Двоичный симметриный канал: \(X, Y \in \{0, 1\}, p_{Y \mid X}(y \mid x) = \begin{cases}
            p, & y \neq x \\
            1 - p , & y = x
    \end{cases}\)
    \[C_{BSC} = 1 + p \log_2 p + (1 - p) \log_2(1 - p)\] 
\item Идеальный частотно ограниченный гауссовский канал: \(y(t) = x(t) + \eta(t), \eta(t)\) --- гауссовский случайный процесс, спектральная плотность  мщности которого равна \(S(f) = \begin{cases}
            \frac{N_0}{2}, & -W < f < w \\
            0, & \text{иначе}
    \end{cases}\)
    Из теории случайных процессов:
    \[C_{AWGN} = W \log_2 \left(1 + \frac{E_S}{WN_0}\right)\] 
    \?
\end{enumerate}
