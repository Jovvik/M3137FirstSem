\chapter{9 сентября}

\subsection{Энергетический выигрыш кодирования}

Энергетический выигрыш кодирования показывает,
во сколько раз использование кодирования позволяет уменьшить отношение сигнал/шум,
необходимое для достижения заданной вероятности ошибки
по сравнению со случаем отсутствия кодирования.

Выигрыш от кодирования неидеален, т.к.:
\begin{enumerate}
	\item Оптимальный код сложно придумать (прямая теорема кодирования не конструктивна)
	\item Используются коды конечной (и обычно малой) длины
	\item Алгоритмы декодирования часто декодируют не по максимуму правдоподобия, а как получится
	\item Дискретизация выхода канала
\end{enumerate}

Теория кодирования занимается увеличением энергетического выигрыша кодирования.

\section{Жесткое и мягкое декодирование}

\begin{itemize}
	\item При \textbf{мягком декодировании} декодер непосредственно использует \(y_{i}\),
	      т.е. есть информация о надежности символов.
	\item При \textbf{жестком декодировании} декодер использует только оценки \(\hat{x_{i}}\).
\end{itemize}

\begin{example}
	Канал с аддитивным белым гауссовским шумом (АБГШ): \(y_{i} = (2x_{i} - 1) + \eta_i, x_{i} \in \{0, 1\}\)

	В случае жесткого декодирования канал превращается в двойной симметричный канал
    с вероятностью ошибки \(p = Q\left(\sqrt{2 \frac{E_s}{N_0}}\right)\)

    \begin{remark}
        При малых \(x\) \(Q(x) \approx \frac{1}{2} - \frac{x}{\sqrt{2\pi}}\)
    \end{remark}

    Достижимая скорость передачи данных:
    \[R < \frac{2}{\ln 2} \left(\sqrt{\frac{E_s}{N_0 \pi}}\right)^2\]
    \[\frac{E_b}{N_0} = \frac{E_s}{R N_0} > \frac{\pi}{2} \ln 2 = 1.09 = 0.37 \mathrm{дБ}\]

    Как мы уже выяснили, для мягкого декодирования это значение \( - 1.59 \mathrm{дБ}\), жесткое декодирование хуже.
\end{example}

\begin{example}
    Рассмотрим идеальный частотно ограниченный гауссовский канал,
    т.е. спектральная плотность мощности отличается от нуля только в некотором диапазоне частот.

    Спектральная эффективность кодирования --- \(\beta = \frac{R}{W}\), скорость передачи данных, поделенная на ширину канала.
    \[\frac{R}{W} < \log_2 \left(1 + \frac{R}{W} \frac{E_b}{N_0}\right)\]
    \[\frac{E_b}{N_0} > \frac{2^\beta - 1}{\beta}\]
\end{example}

\subsection{Критерии декодирования}

Оптимальные критерии декодирования сложны в реализации, сведем их к чему-нибудь попроще.

\begin{enumerate}
    \item \textbf{Критерий минимального расстояния}: кодовое слово \(c = \argmin_{c \in C} d(c, y)\)
    \item \textbf{Списочное декодирование}: ищем все кодовые слова,
    находящиеся в сфере заданного радиуса вокруг полученного вектора.
    \item \textbf{Побитовое декодирование}: используется критерий идеального наблюдателя для отдельных символов кодового слова.
    Часто также дополняется вычислением логарифма отношений правдоподобия для отдельных символов.
    \[L_i = \ln \frac{\sum_{\substack{c \in C \\ c_i = 0}} P \{c \mid y\}}{\sum_{\substack{c \in C \\ c_i = 1}} P \{c \mid y\}}\]
\end{enumerate}

aaa

\unfinished
