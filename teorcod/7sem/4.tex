\chapter{23 сентября}

\section{Универсальные методы декодирования линейных кодов}

Мы изучали методы жесткого декодирования, но на практике чаще применяются методы мягкого декодирования, которые учитывают надежность символов.

Мы уже знаем, что декодирование кода по критерию максимального правдоподобия
в канале с АБГШ эквивалентно декодированию по критерию минимального расстояния Евклида.

Несложными преобразованиями получим:
\[\?\]

Пусть тогда \(\hat{c}_i = \begin{cases}
    0, & y_i > 0 \\
    1, & y_i \leq 0
\end{cases}\) --- жесткие решения.

Тогда:
\begin{align}
    \argmax_{c \in C} \sum_{i=0}^{n - 1} ( -1)^{c_i} y_i
    & = \argmax_{c \in C} \sum_{i=0}^{n - 1} ( -1)^{c_i} y_i \\
    & = \? \\
\end{align}

\unfinished

\subsection{Метод порядковых статистик}

Рассмотрим передачу кодовых слов \(c_0 \dots c_{n-1}\) двоичного \((n, k)\) кода
с помощью символов 2-АМ, например \(y_i = ( - 1)^{c_i} + \eta_i\) --- с АБГШ.

Пусть тогда \(L_i = \log \frac{P \{c_i = 0 \mid y_i\}}{P \{c_i = 1 \mid y_i\}}\) --- логарифмические отношения правдоподобия.
Вероятность ошибки в \(i\)-том жестком решении убывает с увеличением \(|L_i|\).
Тогда выберем информационную совокупность \(J\) для кода, соответствующего
наибольшим значениям \(|L_i|\). Приведем порождающую матрицу кода к виду \(G_J\) с единичной подматрицей в столбцах \(J\).

С большой вероятностью число неверных решений \(\hat{c}_j\), где \(j \in J\), мало.
Переберем все конфигурации ошибок \(e\) веса не более \(t\) на \(J\) и построим кодовые слова \(c_e = (\hat{c}_J + e)G_J\).
Из всех полученных кодовых слов выберем наиболее правдоподобное.

Сложность алгоритма \(\mathcal{O}\left(k^2 n + \sum_{i=0}^{t} \mathrm{in} C_k^i\right)\).
% TODO: почему?
% Т.к. мы перебираем конфигурации ошибок

Есть два\footnote{Не взаимозаменяющих} способа ускорить этот алгоритм:
\begin{enumerate}
    \item Обменять память на скорость каким-то образом (придумать дома).
    \item Реализовать раннюю остановку. % TODO: как?
\end{enumerate}

Рассмотрим двоичный \((n, k, d)\) код \(C\) с порождающей матрицей \(G\), пусть \(D(x, y)\) --- функция расстояния Хемминга.

Существует много кодов, содержащих одинаковые префиксы некоторой длины \(a\),
поэтому можно не пересчитывать \(\sum_{i=0}^{a} D(c_i, y_i)\) заново.

\unfinished

\subsection{Декодирование по решеткам}

\begin{definition}
    \textbf{Решетка \textit{(англ. \?)}} --- граф, обладающий следующими свойствами:
    \begin{enumerate}
        \item Вершины графа разбиты на непересекающиеся подмножества, называемые \textbf{уровнями} или \textbf{ярусами}.
        \item Нулевой и последний ярусы содержат по одной вершине, называемой \textbf{терминальной}.
        \item Граф направленный и допускается движение только от уровня с меньшим номером к уровню с большим номером.
        \item Ребрам графа сопоставлены метки, соответствующие символам кодовых слов,
        а также метрики, называемые весами.
    \end{enumerate}
\end{definition}

На таком графе можно запустить алгоритм Дейкстры или алгоритм Витерби % TODO: в чем разница, написать алгос

\begin{definition}
    \textbf{Профиль сложности решетки} это \(\xi_0 \dots \xi_n\), где \(\xi_i = |V_i|\).
\end{definition}

\begin{definition}
    Решетка называется \textbf{минимальной}, если профиль сложности решетки минимален среди всех решеток с заданным количеством ярусов.
\end{definition}

Рассмотрим все кодовые слова \(c_m = c_{m,0} \dots c_{m,n - 1}\) кода.

Для любого \(i\) определим префикс длины \(i\), называемый \textbf{прошлым} и обозначим его \(c_m^p\), а оставшийся суффикс длины \(n - i\) --- \textbf{будущим} и обозначим его \(c_m^f\).

Очевидно, что в произвольной решетке пути, входящие в фиксированную вершину, имеют общее будущее, а пути, исходящие из фиксированной вершины, имеют общее прошлое. % TODO: не очевидно же блет

\unfinished

Докажем, что построенная решетка минимальна.
\begin{proof}
    Рассмотрим произвольную решетку \(T'\) этого кода.

    В \(T'\) два слова \(c_1 = (c_1^p, c_1^f)\) и \(c_2 = (c_2^p, c_2^f)\) могут иметь общую вершину на ярусе \(i\) только если \(F_i(c_1^p) = F_i(c_2^p)\).
    По построению два пути, проходящие через общую вершину в \(T'\), проходят также через общую вершину в \(T\).

    Таким образом, число вершин на ярусе \(i\) в \(T'\) не меньше числа вершин на ярусе \(i\) в \(T\).
\end{proof}

\begin{theorem}
    Любой код имеет минимальную решетку, и все минимальные решетки совпадают с точностью до нумерации вершин яруса.
\end{theorem}

\unfinished

\begin{theorem}
    Решетка, получаемая по порождающей матрице в минимальной спановой форме, минимальна.
\end{theorem}
\begin{proof}
    Докажем, что для любого \(l \in \N\) пути, определяющие слова с одинаковыми \(c^f\) длины \(n - l\), не проходят через различные узлы на ярусе с номером \(l\).

    Узел, через который проходит путь на ярусе \(l\), определяется значениями информационных символов, которые соответствуют активным на этом ярусе строкам. Т.к. эти строки линейно независимы и заканчиваются на ярусах с номерами \( > l\), следовательно, нетривиальные линейные комбинации этих строк отличаются хотя бы на одной позиции \( > l\).

    Предположим, что есть два слова с одинаковым будущим, проходящие через разные узлы на ярусе \(l\).
    Их сумма образует слово, активное на ярусе \(l\) и равное \(0\) на позициях правее \(l\).
    Но из соображений выше таких слов быть не может,
    а следовательно слова с одинаковым будущим проходят через одни и те же узлы,
    а следовательно решетка минимальна.
\end{proof}

\unfinished

\begin{theorem}
    Решетка, построенная по проверочной матрице, минимальна.
\end{theorem}
\begin{proof}
    Докажем, что пути с одинаковыми \(c^f\) не проходят через разные узлы.

    Для кодового слова \(c = (c^p, c^f)\) частичные синдромы,
    вычисленные по \(c^p\) и \(c^f\), совпадают.
    Следовательно, все совпадающие \(c^f\) исходят из одного и того же узла, определенного частичным синдромом \(c^p\).
\end{proof}

\section{Декодирование с мягким выходом}

Длинные коды можно строить путём комбинирования более коротких кодов. Как --- мы узнаем позже.
Декодеры таких кодов могут быть построены из декодеров кодов-компонент.
Такие декодеры могут взаимодействовать путем обмена апостериорными вероятностями:
\[p \{c_i = a \mid y_0^{n -1}\} = \sum_{c \in C_i(a)} p \{c \mid y_0^{n -1}\}\]
\?

\subsection{Алгоритм Бала--Коке--Елинека--Равива}

Нужно вычислить
\[L_i = \ln \frac{P \{c_i = 0 \mid y_0^{n -1}\}}{P \{c_i = 1 \mid y_0^{n -1}\}}
 = \ln \frac{\sum_{(s', s) \in S_0} \cfrac{p(s_i = s', s_{i+1}, y_0^{n - 1})}{p(y_0^{n -1})}}{\sum_{(s', s) \in S_0} \cfrac{p(s_i = s', s_{i+1}, y_0^{n - 1})}{p(y_0^{n -1})}}\]
, где \(S_0\) и \(S_1\) --- множества пар состояний \(s' \in V_i, s \in V_{i+1}\),
переход между которыми помечен \(0\) и \(1\) соответственно,
\(p(y_0^{n -1})\) --- совместная плотность распределения принятых сигналов,
\(p(s_i = s', s_{i+1} = s, y_0^{n - 1})\) --- совместная плотность распределения принятых сигналов и состояний кодера на ярусах \(i\) и \(i + 1\).

Поведение кодера при обработке \(i\)-го информационного бита определяется
только его состоянием \(s'\) на предыдущем шаге и канал не имеет памяти:
\begin{align}
    p(s_i = s', s_{i+1} =s, y_0^{n - 1})
    & = p(s_i = s', y_0^{n - 1}) p(s_{i+1} = s, y_i \mid s_i = s', y_0^{y - 1}) p(y_{i+1}^{n -1} \mid s_{i+1} = s, s_i = s', y_0^i) \\
    & = \underbrace{p(s_i = s', y_0^{n - 1})}_{\alpha_i(s')} \underbrace{p(s_{i+1} = s, y_i \mid s_i = s')}_{\gamma_{i+1}(s',s)} \underbrace{p(y_{i+1}^{n -1} \mid s_{i+1} = s)}_{\beta_{i+1}(s)}
\end{align}
\begin{remark}
    Предыдущая строка --- определения \(\alpha, \beta, \gamma\)
\end{remark}
% TODO: записать объяснение с 5 лекции