\chapter{14 октября}

\section{Полярные коды}

\subsection{Некоторые определения}

\subsubsection{Функция переходных вероятностей канала}

\begin{definition}
Рассмотрим канал без памяти с входным алфавитом \(\mathcal{X} = \{0, 1\}\) и
выходным алфавитом \(\mathcal{Y}\).

Если \(\mathcal{Y}\) дискретен, то \textbf{функция переходных вероятностей канала} \(W(y \mid c)\)
--- вероятность наблюдения на выходе \(y \in \mathcal{Y}\)
при условии подачи на его вход \(c \in \mathcal{X}\).

Если \(\mathcal{Y}\) непрерывен, то \textbf{функция переходных вероятностей канала} \(W(y \mid c)\)
--- плотность распределения выходного символа при подаче \(c\) на его вход
\end{definition}

\begin{example}[Двоичный симметричный канал]
    \[\mathcal{Y} = \mathcal{X} \quad W(y \mid c) = \begin{cases}
        p, & y \neq x \\
        1 - p, & y = x
    \end{cases}\]
\end{example}

\begin{example}[Двоичный стирающий канал]
    \[\mathcal{Y} = \{0, 1, \varepsilon\} \quad W(y \mid c) = \begin{cases}
        p, & y = \varepsilon \\
        1 - p, & y = x \in \{0, 1\}
    \end{cases}\]
\end{example}

\begin{example}[Двоичный симметричный канал со стираниями]
    \[\mathcal{Y} = \{0, 1, \varepsilon\} \quad W(y \mid c) = \begin{cases}
        1 - p - s, & y = x \\
        s, & y = \varepsilon \\
        p, & y \neq x, y \neq \varepsilon
    \end{cases}\]
\end{example}

\begin{example}[Аддитивный гауссовский канал]
    \[\mathcal{Y} = \R, y = ( - 1)^c + \eta, \eta \sim \mathcal{N}(0, \sigma^2) \quad W(y \mid c) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left( - \frac{(y - ( - 1)^c)^2}{2\sigma^2}\right)\]
\end{example}

\subsubsection{Параметр Бхаттачарьи}

Рассмотрим канал без памяти с двоичным кодом, приемник по максимуму правдоподобия.

Если передаваемые символы равновероятны, то вероятность ошибки:
\begin{align*}
    P_e
     & = P \{c = 0\} P \{\mathrm{err} \mid c = 0\} + P \{c = 1\} P \{\mathrm{err} \mid c = 1\} \\
     & = \frac{1}{2} \sum_{y : W(y \mid 0) < W(y \mid 1)} W(y \mid 0) + \frac{1}{2} \sum_{y : W(y \mid 1) < W(y \mid 0)} W(y \mid 1) \\
     & = \frac{1}{2} \sum_{y : \frac{W(y \mid 1)}{W(y \mid 0)} > 1} W(y \mid 0) + \frac{1}{2} \sum_{y : \frac{W(y \mid 0)}{W(y \mid 1)} > 1} W(y \mid 1) \\
     & = \frac{1}{2} \sum_{y \in \mathcal{Y}} \sum_{c \in \{0, 1\}} \left(W(y \mid c) \chi\left(\frac{W(y \mid 1 - c)}{W(y \mid c)}\right)\right)
\end{align*}
, где \(\chi\) --- индикаторная функция:
\[\chi(z) = \begin{cases}
    1, z \geq 1 \\
    0, z < 1
\end{cases}\]

Заметим, что \(\chi(z) \leq \sqrt{z}\) для всех \(z \in \N\).
Таким образом, можно оценить вероятность ошибки:
\begin{align*}
    P_e
     & \leq \frac{1}{2} \sum_{y \in \mathcal{Y}} \sum_{c \in \{0, 1\}} W(y \mid c) \sqrt{\frac{W(y \mid 1 - c)}{W(y \mid c)}} \\
     & = \frac{1}{2} \sum_{y \in \mathcal{Y}} \sum_{c \in \{0, 1\}} \sqrt{W(y \mid 1 - c)W(y \mid c)} \\
     & = \sum_{y \in \mathcal{Y}} \sqrt{W(y \mid 1)W(y \mid 0)} \\
     & \eqqcolon Z(W)
\end{align*}

\begin{definition}
    \(Z(W)\) --- \textbf{параметр Бхаттачарьи} ФПВК \(W\).
\end{definition}

\begin{example}[Двоичный стирающий канал]
    \[Z(BEC(p)) = \sqrt{W(0 \mid 0)W(0 \mid 1)} + \sqrt{W(1 \mid 0)W(1 \mid 1)} + \sqrt{W(\varepsilon \mid 0)W(\varepsilon \mid 1)} = 0 + 0 + p = p\]
\end{example}

\begin{example}[Аддитивный гауссовский канал]
    \begin{align*}
        Z(\mathcal{G}(\sigma))
        & = \int_{ - \infty}^{\infty} \sqrt{W(y \mid 0)W(y \mid 1)}dy \\
        & = \frac{1}{\sqrt{2\pi\sigma^2}} \int_{ - \infty}^{\infty} \exp( - \frac{(y - 1)^2 + (y + 1)^2}{4\sigma^2})dy \\
        & = \exp( - \frac{1}{2\sigma^2})
    \end{align*}
\end{example}

Пропускную способность канала можно вычислить по формуле:
\[I(W) = \max_{\{p(x)\}} \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} W(y \mid x) P \{x\} \log \frac{W(y \mid x)}{W(y)}\]

Для многих каналов оптимальным распределением символов на входе \(P \{x\}\) является равномерное,
что мы и будем рассматривать дальше.

\subsection{Поляризация канала}

Рассмотрим следующее линейное преобразование:
\[\mqty(c_0 & c_1) = \mqty(u_0 & u_1) \mqty(1 & 0 \\ 1 & 1)\]

Пропустим полученные символы через двоичный стирающий канал с вероятностью стирания \(p\),
получим \(y_0, y_1\).

Если оба символа не стерты, то \(u_0\) можно восстановить из \(y_0, y_1\) как \(y_0 \xor y_1\).
Вероятность того, что этого не произойдет --- \(1 - (1 - p)^2 = 2p - p^2 \geq p\)

\(u_1\) восстанавливается либо как \(y_1\), либо как \(y_0 \xor u_0\) и тогда нам нужна подсказка свыше о \(u_0\).
Вероятность того, что восстановить не получится
--- вероятность того, что оба символа стерты, т.е. \(p^2 \leq p\).

Таким образом, мы с помощью \textbf{поляризации} получили два виртуальных канала
--- один чуть получше, другой чуть похуже, чем исходный канал.

Пусть \(n = 2^m, A_m = \mqty(1 & 0 \\ 1 & 1)^{\xor m}\), где \(\xor m\) ---
\(m\)-кратное произведение Кронекера матрицы с собой.
Т.к. канал без памяти, выполняется \(W_m(y^{n-1}_0 \mid c^{n-1}_0) = \prod_{i = 0}^{n-1} W(y_i \mid c_i)\).

Вход канала --- \(u_i\), выход --- реальный выход канала \(y^{n-1}_0\)
и подсказка свыше о предыдущих символах \(u^{i-1}_0\).

Определим ФПВ для синтетических каналов:
\begin{align*}
    W_m^{(i)}(y^{n-1}_0, u^{i-1}_0 \mid u_i)
    & \defeq \frac{W_m^{(i)}(y^{n-1}_0, u^i_0)}{P \{u_i\}} \\
    & = \frac{\sum_{u_{i+1}^{n - 1} \in \mathbb{F}_2^{n - i}} W_m^{(i)}(y^{n-1}_0 \mid u^{n-1}_0) P \{u^{n-1}_0\}}{\frac{1}{2}} \\
    & = 2 \sum_{u_{i+1}^{n - 1} \in \mathbb{F}_2^{n - i}} W_m^{(i)}(y^{n-1}_0 \mid u^{n-1}_0) P \{u^{n-1}_0\} \\
    & = \frac{2}{2^n} \sum_{u_{i+1}^{n - 1} \in \mathbb{F}_2^{n - i}} W_m^{(i)}(y^{n-1}_0 \mid u^{n-1}_0) \\
    & = \frac{2}{2^n} \sum_{u_{i+1}^{n - 1} \in \mathbb{F}_2^{n - i}} W_m^{(i)}(y^{n-1}_0 \mid u^{n-1}_0) \\
    & = \frac{2}{2^n} \sum_{u_{i+1}^{n - 1} \in \mathbb{F}_2^{n - i}} W_m(y^{n-1}_0 \mid u^{n-1}_0 A_m) \\
    & = 2^{1 - n} \sum_{u_{i+1}^{n - 1} \in \mathbb{F}_2^{n - i}} \prod_{j = 0}^{n - 1} W(y_j \mid (u^{n-1}_0 A_m)_j)
\end{align*}

Вычислим ФПВ битовых подканалов, для начала для простейшего случая \(n = 0\):
\begin{align*}
    W_1^{(0)}(y_0, y_1 \mid u_0)
    & = \frac{W_1^{(0)}(y_0,y_1,u_0)}{P \{u_0\}} \\
    & = 2 \sum_{u_1 = 0}^1 W_1^{(1)}(y_0,y_1,u_0,u_1) \\
    & = 2 \sum_{u_1 = 0}^1 W_1^{(1)}(y_0,y_1 \mid u_0,u_1) P \{u_0,u_1\} \\
    & = \frac{1}{2} \sum_{u_1 = 0}^1 W(y_0 \mid u_0 +u_1) + W(y_1 \mid u_1)
\end{align*}
\[W_1^{(0)}(y_0,y_1 \mid u_0) = \frac{1}{2} \sum_{u_1 = 0}^1 W(y_0 \mid u_0 + u_1) W(y_1 \mid u_1)\]
\[W_1(y_0,y_1,u_0 \mid u_1) = \frac{1}{2} W(y_0 \mid u_0 + u_1) W(y_1 \mid u_1)\]

Из тех же самых соображений получаем рекурсивное определение:
\[W_\lambda^{(2i)}(y_0^{2^\lambda - 1}, u_0^{2i - 1} \mid u_{2i}) = \frac{1}{2} \sum_{u_{2i + 1} = 0}^1 W_{\lambda - 1}^{(i)} (y_{0,even}^{2^\lambda - 1},u_{0,even}^{2i - 1} + u_{0,odd}^{2i - 1} \mid u_{2i} + u_{2i + 1}) W^{(i)}_{\lambda - 1}(y_{0,odd}^{2^\lambda - 1},u_{0,odd})\]
\[W_\lambda^{(2i + 1)}(y_0^{2^\lambda - 1}, u_0^{2i} \mid u_{2i + 1}) = \frac{1}{2} W_{\lambda - 1}^{(i)} (y_{0,even}^{2^\lambda - 1},u_{0,even}^{2i - 1} + u_{0,odd}^{2i - 1} \mid u_{2i} + u_{2i + 1}) W^{(i)}_{\lambda - 1}(y_{0,odd}^{2^\lambda - 1},u_{0,odd}^{2i - 1} \mid u_{2i + 1})\]

\subsubsection{Параметры подканалов}

Параметры Бхаттачарьи для битовых подканалов \(Z_{m,i} = Z(W_m^{(i)})\) (без доказательства):
\[Z_{m,2i + 1} \leq Z_{m,2i} \leq 2Z_{m - 1,i} - Z_{m - 1,i}^2\]
\[Z_{m, 2i + 1} = Z_{m - 1,i}^2\]
При этом в случае двоичного стирающего канала равенство, а не \( \leq\).

Пропускные способности:
\[I_{m,2i} + I_{m, 2i + 1} = 2I_{m - 1,i}\]
\[I_{m, 2i} \leq I_{m, 2i + 1}\]

И еще пропускные способности связаны с параметрами Бхаттачарьи следующим образом:
\[\sqrt{1 - Z(W)^2} \geq I(W) \geq \log \frac{2}{1 + Z(W)}\]

\begin{theorem}
    Для любого \(\delta \in (0, 1)\) при \(m \to \infty\) доля подканалов с
    \(I(W_m^{(i)}) \in (1 - \delta, 1]\) стремится к \(I(W_0^{(0)}) = I(W)\),
    т.е. пропускной способности исходного канала,
    а доля подканалов с \(I(W_m^{(i)}) \in [0, \delta)\) стремится к \(1 - I(W)\).

    Т.е. синтетические подканалы сходятся или к очень хорошим, или очень плохим каналам.

    Это верно не только для двоичного стирающего канала.
\end{theorem}

\subsection{Полярный код}

Будем посылать по плохим каналам предопределенные символы, например нули.
По остальным каналам посылаем полезные данные.
Тогда кодирование имеет вид \(c_0^{n - 1} = u^{n-1}_0 A_m\), где \(u_i = 0\) при \(i \in \mathcal{F}\)
--- множество плохих каналов или соответственно замороженных символов.

Таким образом мы получаем линейный блоковый код длины \(2^m\) с размерностью \(2^m - |\mathcal{F}|\).

Декодирование выполняется алгоритмом последовательного исключения:
\begin{enumerate}
    \item Для замороженных символов значения известны.
    \item Для не замороженных символов используется оценка
    \(\hat{u}_i = \argmax_{u_i} W_m^{(i)}(y^{n-1}_0, \hat{u}^{i-1}_0 \mid u_i)\).
    В формулу надо бы подставить истинные значения \(u^{i-1}_0\), но мы их не знаем.
    Если \(u^{i-1}_0\) было найдено верно, то мы получим верное значение \(u_i\),
    а если мы ошиблись, то мы и так уже проиграли.
\end{enumerate}

Вероятность ошибки можно оценить как:
\[P \leq \sum_{i \notin F} Z_{m,i} \leq 2^{ - n^\beta}, \beta < 0.5\]

Указать точное число исправимых ошибок тяжело, и не очень полезно,
потому что бывает такое: гарантированно исправляем три ошибки, но с высокой вероятностью исправим и 10.

Чтобы кодировать, нужно умножать \(u\) на \(A_m\), т.е.:
\[u^{n-1}_0 A_m = \mqty(u^{n / 2 - 1}_0 & u_{n / 2}^{n - 1}) \mqty(A_{m - 1} & 0 \\ A_{m - 1} & A_{m - 1})
= \mqty((u_0^{n / 2 - 1} + u_{n / 2}^{n - 1}) A_{m - 1} & u_{n / 2}^{n - 1} A_{m - 1})\]
Тогда сложность кодирования:
\[T(n) = \underbrace{2T\left(\frac{n}{2}\right)}_{\text{Умножение на матрицу}} + \underbrace{\frac{n}{2}}_{\text{Сложение векторов}} = \frac{1}{2} n \log_2 n\]

Определим логарифмическое отношение правдоподобия:
\[L_m^{(i)}(y^{n-1}_0, u^{i-1}_0) \coloneqq \ln \frac{W_m^{(i)}(y^{n-1}_0,u^{i-1}_0 \mid 0)}{W_m^{(i)}(y^{n-1}_0,u^{i-1}_0 \mid 1)}\]

Тогда подстановкой рекурсивного определения \(W_m^{(i)}\) можно получить для нечетных каналов:
\[L_\lambda^{(2i + 1)}(y^{n-1}_0, u_0^{2i})
= \?
= ( - 1)^{u_{2i}} L_{\lambda - 1}(y^{2^\lambda - 1}_{0,even}, u_{0,even}^{2i - 1} + u_{0,odd}^{2i - 1})
    + L_{\lambda - 1}(y_{0,odd}^{2^\lambda - 1}, u_{0, odd}^{2i - 1})\]

Пусть при этом:
\begin{align*}
    p_s & = W_\lambda^{(2i)}(s \mid y_0^{2^\lambda - 1}, u_0^{2i - 1}) = \frac{W_\lambda^{(2i)}(y_0^{2^\lambda - 1}, u_0^{2i - 1} \mid s)}{2 W_\lambda^{(2i)}(y_0^{2^\lambda - 1}, u_0^{2i - 1})} \\
    p_{0s} & = W_{\lambda - 1}^{(i)}(s \mid y_{0,even}^{2^\lambda - 1},u_{0,even}^{2i - 1} + u_{0,odd}^{2i - 1}) \\
    p_{1s} & = W_{\lambda - 1}^{(i)}(s \mid y_{0,odd}^{2^\lambda - 1}, u_{0,odd}^{2i - 1})
\end{align*}

Тогда рекурсивные формулы записываются как:
\begin{align*}
    p_0 & = p_{00} p_{10} + p_{01} p_{11} \\
    p_1 & = p_{01} p_{10} + p_{00} p_{11}
\end{align*}
, причем:
\begin{align*}
    p_0 + p_1 & = 1 \\
    p_{i 0} + p_{i 1} & = 1
\end{align*}

Забавный факт:
\begin{align*}
	\tanh(\frac{1}{2}\ln \frac{p_0}{p_1})
    & \defeq \frac{\exp(\ln(\frac{p_0}{p_1}))- 1}{\exp(\ln(\frac{p_0}{p_1})) + 1} \\
    & = \frac{\frac{p_0}{p_1} - 1}{\frac{p_0}{p_1} + 1} \\
    & = \frac{p_0 - p_1}{p_0 + p_1} \\
    & = p_0 - p_1 \\
    & = 1 - 2p_1 \\
    & = (1 - 2p_{01})(1 - 2p_{11}) \\
    & = 1 - 2(p_{01} + p_{11} - 2p_{11}p_{01}) \\
    & = 1 - 2(p_{01}(1 - p_{11}) + (1 - p_{01})p_{11})
\end{align*}

% Итого:
% \begin{align*}
%     1 - 2p_1 & = 1 - 2(p_{01}(1 - p_{11}) + (1 - p_{01})p_{11}) \\
%     % \tanh(\frac{1}{2}L_\lambda^{(2i)}(y^{n-1}_0, u^{2i - 1}_0))
%     %     & = \tanh(\frac{1}{2}L_{\lambda - 1}^{(i)}(y_{0,even}^{2^\lambda - 1},u_{0,even}^{2i - 1} + u_{0,odd}^{2i - 1})) \tanh(\frac{1}{2}L_{\lambda - 1}^{(i)}(y_{0,even}^{(2^\lambda - 1)},u_{0,even}^{2i - 1} + u_{0,odd}^{2i - 1})) \\
% \end{align*}

\?

Алгоритм последовательного исключения:
\[\hat{u}_i = \begin{cases}
    0, & i \in \mathcal{F} \\
    0, & L_m^{(i)}(y^{n-1}_0,\hat{u}^{i-1}_0) > 0, i \notin \mathcal{F} \\
    1, & L_m^{(i)}(y^{n-1}_0,\hat{u}^{i-1}_0) \leq 0, i \notin \mathcal{F}
\end{cases}\]

Есть также другой вариант алгоритма, где поменяны местами \(u_0^i\) и \(y^{n-1}_0\):
\[\begin{gathered}
W_m^{(i)}\left(u_0^i \mid y_0^{n-1}\right)=\frac{W_m^{(i)}\left(y_0^{n-1}, \widehat{u}_0^{i-1} \mid u_i\right)}{2 W\left(y_0^{n-1}\right)}=\sum_{u_{i+1}^{n-1}} \prod_{j=0}^{n-1} W\left(\left(u_0^{n-1} A_m\right)_j \mid y_j\right) \\
W_\lambda^{(2 i)}\left(u_0^{2 i} \mid y_0^{n-1}\right)=\sum_{u_{2 i+1}} W_{\lambda-1}^{(i)}\left(u_{2 t}+u_{2 t+1}, 0 \leq t \leq i \mid y_{0, \text { even }}^{n-1}\right) W_{\lambda-1}^{(i)}\left(u_{2 t+1}, 0 \leq t \leq i \mid y_{0, \text { odd }}^{n-1}\right) \\
W_\mu^{(2 i+1)}\left(u_0^{2 i+1} \mid y_0^{n-1}\right)=W_{\lambda-1}^{(i)}\left(u_{2 t}+u_{2 t+1}, 0 \leq t \leq i \mid y_{0, \text { even }}^{n-1}\right) W_{\lambda-1}^{(i)}\left(u_{2 t+1}, 0 \leq t \leq i \mid y_{0, \text { odd }}^{n-1}\right)
\end{gathered}\]

Если мы хотим получить код размерности \(k\),
то необходимо заморозить \(2^m - k\) наименн надежных символов,
например с наибольшим \(Z_{m,i}\).

В случае двоичного стирающего канала \(Z_{m,i}\) вычисляется просто --- рекурсией с мемоизацией,
со сложностью \(\mathcal{O}(n)\).

В общем случае выходной алфавит канала \(W_m^{(i)}(y^{n-1}_0, u^{i-1}_0 \mid u_i)\)
имеет мощность \(|\mathcal{Y} |^n 2^i\), а поэтому выписывать (даже не вычислять)
все вероятности нереализуемо за адекватное время.
Однако можно аппроксисмировать \(W_m^{(i)}\) каналом с выходным алфавитом фикированной мощности,
который чуть лучше или хуже, чем настоящий канал.
С помощью этого метода \(Z_{m,i}\) можно вычислить за \(\mathcal{O}(n \mu^2 \log\mu)\),
где \(\mu\) --- мощность выходного алфавита канала.

Можно поступить проще, заметив, что для симметричных каналов
вероятность ошибки не зависит от того, какое кодовое слово передавалось,
а т.к. полярные коды линейные, то нулевое слово является кодовым.
Тогда будем считать, что передавалось нулевое слово.

\begin{example}
    При передаче кодовых слов по аддитивному гауссовскому каналу выполнено
    \(L_0^{(0)}(y_i) = \frac{2y_i}{\sigma^2}\).
    Т.к. мы рассматриваем нулевое кодовое слово, то получается, что
    \(M[L_0^{(0)}(y_i)] = \mu_{00} = \frac{2}{\sigma^2}\) и \(D[L_0^{(0)}(y_i)] = \frac{4}{\sigma^2} = 2M\).
\end{example}

Предположим, что все логарифмические отношения правдподобий имеют нормальное распределение\footnote{Это неверно в общем случае, но аппроксимация все равно получается неплохая.}
\[\mathcal{L}_\lambda^{(i)} \sim \mathcal{N}(\mu_{\lambda,i}, 2\mu_{\lambda,i}), \quad 0 \leq i < 2^\lambda, 0 \leq \lambda \leq m\]

Можем получить рекурсивную формулу для матожиданий (без доказательства):
\begin{align*}
    \mu_{\lambda, 2i} & = \Theta(\mu_{\lambda - 1,i}) = \phi(1 - (1 - \phi(\mu_{\lambda - 1,i}))^2) \\
    \mu_{\lambda,2i + 1} & = 2 \mu_{\lambda - 1, i}
\end{align*}
, где:
\[\phi(x) = 1 - \frac{1}{\sqrt{4\pi x}} \int_{ - \infty}^{\infty} \tanh \frac{u}{2} \exp( - \frac{(u - x)^2}{4x}) \dd{u}\]

Замораживаются символы с наименьшим \(\mu_{m,i}\).

\(\Theta\) считать долго, поэтому есть аппроксимация:
\[\Theta(x) \approx\left\{\begin{array}{lr}
0.9861 x-2.3152, & x>12 \\
x(9.005 \cdot 10^{-3} x+0.7694)-0.9507, & x \in(3.5,12] \\
x(0.062883 x+0.3678)-0.1627, & x \in(1,3.5] \\
x(0.2202 x+0.06448), & \text { otherwise }
\end{array}\right.\]

Несмотря на свое определение,
код Рида-Маллера \(RM(r, m)\) длины \(2^m\) порядка \(r\) --- полярный код с
\[\mathcal{F} = \{i \mid 0 \leq i < 2^m, \wt(i) < m - r\}\]
\begin{proof}
    Рассмотрим \(m = 1\).
    Тогда \(RM(0, 1)\) --- код \((2, 1, 2)\) и \(RM(1, 1)\) --- код \((2, 2, 1)\)
    с порождающими матрицами \(\mqty(1 & 1)\) и \(\mqty(1 & 0 \\ 1 & 1)\) соответственно.
    Несложно заметить, что искомое выполнено.

    Индукицонный переход: пусть искомое верно для \(RM(r, m - 1)\) и \(RM(r - 1, m)\).
    Тогда пусть
    \[\mathcal{F}_{r, m} = F_{r,m - 1} \cup \{2^{m - 1} + i \mid i \in \mathcal{F}_{r - 1,m}\}\]

    Тогда \(wt(i) < m - 1 - r + 1 = m - 1\).
\end{proof}